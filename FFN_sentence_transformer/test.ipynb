{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from utils.adapter import BERTAdapter\n",
    "from utils.feed_forward import FeedForward\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set Device ##########################################################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = load_dataset(\"glue\", \"cola\")\n",
    "\n",
    "# Initialize the tokenizer from the BERT base uncased model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "mpnetv2 = SentenceTransformer(\"all-mpnet-base-v2\").to(device)\n",
    "\n",
    "# Define a function to tokenize a batch of texts\n",
    "def encode_batch(batch):\n",
    "    \"\"\"Tokenizes a batch of texts using the pre-initialized tokenizer.\"\"\"\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "X_train = mpnetv2.encode(data[\"train\"][\"sentence\"])\n",
    "X_val = mpnetv2.encode(data[\"validation\"][\"sentence\"])\n",
    "X_test = mpnetv2.encode(data[\"test\"][\"sentence\"])\n",
    "\n",
    "Y_train = np.array(data[\"train\"][\"label\"])\n",
    "Y_val = np.array(data[\"validation\"][\"label\"])\n",
    "Y_test = np.array(data[\"test\"][\"label\"])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = data.map(encode_batch, batched=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_adapter = BERTAdapter(\n",
    "        dataset=tokenized,\n",
    "        num_epochs=10,\n",
    "        batch_size=128,\n",
    "        learning_rate=3e-4,\n",
    "        category='C',\n",
    "        label='label',\n",
    "        device='cpu'\n",
    ")\n",
    "X_train, X_val, X_test\n",
    "bert_adapter.fit(X=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformer + FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_epochs': [100],\n",
    "    'batch_size': [32, 128, 512],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    'category': ['C'],\n",
    "    'norm': [False],\n",
    "    'size': [768],\n",
    "    'num_layers': [1, 3, 5, 10],\n",
    "    'weight_decay':[1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    'patience': [3],\n",
    "    'min_delta': [0],\n",
    "    'device': ['cuda:0']\n",
    "}\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "highest_val_accuracy = 0\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for params in all_params:\n",
    "    print(\"Training with parameters:\", params)\n",
    "    # Initialize the model with current set of hyperparameters\n",
    "    feed_forward = FeedForward(**params)\n",
    "    \n",
    "    _, _, val_accuracy = feed_forward.fit(X_train, Y_train, X_val, Y_val)\n",
    "    print(\"Validation accuracy:\", val_accuracy)\n",
    "    \n",
    "    # Save the parameters if they provide a better accuracy\n",
    "    if val_accuracy > highest_val_accuracy:\n",
    "        highest_val_accuracy = val_accuracy\n",
    "        best_params = params\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Highest Validation Accuracy:\", highest_val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
