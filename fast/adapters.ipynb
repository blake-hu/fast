{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapter Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import itertools\n",
    "import pathlib\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaConfig, TrainingArguments, EvalPrediction, default_data_collator\n",
    "from adapters import AutoAdapterModel, AdapterTrainer, BnConfig, PrefixTuningConfig, PromptTuningConfig, ConfigUnion,  ParBnConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = \"cpu\"  # Default device is CPU\n",
    "if torch.cuda.is_available():\n",
    "    if 'COLAB_GPU' in os.environ: # Detects if notebook is being run in a colab environment\n",
    "        print(\"colab environment\")\n",
    "        device_name = \"cuda\" # if you're using a T4 GPU on Colab, the device name should be \"cuda\"\n",
    "    else:\n",
    "        device_name = \"cuda:0\" # CUDA for NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapter_type options: bottleneck, prefix, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_param = \"sst2\"\n",
    "adapter_type = \"bottleneck\"\n",
    "running_test_set = False\n",
    "output_dir = 'adapter-distillroberta-base'\n",
    "\n",
    "param_grid_bottleneck_literature = {\n",
    "    'learning_rate': [3e-5, 3e-4, 3e-3],\n",
    "    'num_train_epochs': [3, 20],\n",
    "    'per_device_train_batch_size': [32],\n",
    "    'per_device_eval_batch_size': [32],\n",
    "    'bn_reduction_factor': [8, 64, 256],\n",
    "}\n",
    "\n",
    "param_grid_prefix_literature = {\n",
    "    'learning_rate': [5e-3, 7e-3, 1e-2, 1e-4],\n",
    "    'num_train_epochs': [3, 20],\n",
    "    'per_device_train_batch_size': [32],\n",
    "    'per_device_eval_batch_size': [32],\n",
    "    'prefix_length': [20, 60, 100],\n",
    "}\n",
    "\n",
    "param_grid_test = {\n",
    "    'learning_rate': [0.0001],\n",
    "    'num_train_epochs': [3],\n",
    "    'per_device_train_batch_size': [32],\n",
    "    'per_device_eval_batch_size': [32],\n",
    "    # 'bn_reduction_factor': [8],\n",
    "    'prefix_length': [60],\n",
    "}\n",
    "\n",
    "if running_test_set:\n",
    "    param_grid = param_grid_test\n",
    "elif adapter_type == \"bottleneck\":\n",
    "    param_grid = param_grid_bottleneck_literature\n",
    "elif adapter_type == \"prefix\":\n",
    "    param_grid = param_grid_prefix_literature\n",
    "\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "print(f\"{len(all_params)} hyperparameter combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskConfig = namedtuple(\"TaskConfig\", [\"sentence_type\", \"class_type\", \"num_classes\", \"col_names\"])\n",
    "\n",
    "task_configs = {\n",
    "    \"cola\": TaskConfig(\"one\", \"BC\", 2, ['sentence']),\n",
    "    \"sst2\": TaskConfig(\"one\", \"BC\", 2, ['sentence']),\n",
    "    \"mrpc\": TaskConfig(\"two\", \"BC\", 2, ['sentence1', 'sentence2']),\n",
    "    \"stsb\": TaskConfig(\"two\", \"R\", None, ['sentence1', 'sentence2']),\n",
    "    \"qqp\": TaskConfig(\"two\", \"BC\", 2, ['question1', 'question2']),\n",
    "    \"mnli_matched\": TaskConfig(\"two\", \"MC\", 3, ['premise', 'hypothesis']),\n",
    "    \"mnli_mismatched\": TaskConfig(\"two\", \"MC\", 3, ['premise', 'hypothesis']),\n",
    "    \"qnli\": TaskConfig(\"two\", \"BC\", 2, ['question', 'sentence']),\n",
    "    \"rte\": TaskConfig(\"two\", \"BC\", 2, ['sentence1', 'sentence2']),\n",
    "    \"wnli\": TaskConfig(\"two\", \"BC\", 2, ['sentence1', 'sentence2']),\n",
    "}\n",
    "\n",
    "task_config = task_configs[task_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_param == \"mnli_matched\": \n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_matched\"\n",
    "    test_key = \"test_matched\"\n",
    "elif task_param == \"mnli_mismatched\":\n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_mismatched\"\n",
    "    test_key = \"test_mismatched\"\n",
    "else:\n",
    "    data = load_dataset(\"glue\", task_param)\n",
    "    val_key = \"validation\"\n",
    "    test_key = \"test\"\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "max_len = 512 # TODO: How is this value decided?\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[task_config.col_names[0]],\n",
    "                     add_special_tokens=True, \n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=max_len,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "def tokenize_double(examples):\n",
    "    return tokenizer(examples[task_config.col_names[0]],\n",
    "                     examples[task_config.col_names[1]],\n",
    "                     add_special_tokens=True,\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=max_len,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "# Tokenize the input\n",
    "data = data.map(tokenize, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "data = data.rename_column(original_column_name=\"label\", new_column_name=\"labels\") \n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Adapter Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_config.class_type == \"R\":\n",
    "    config = RobertaConfig.from_pretrained(\"distilroberta-base\")\n",
    "else:\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        \"distilroberta-base\",\n",
    "        num_labels=task_config.num_classes)\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"distilroberta-base\",\n",
    "    config=config).to(device)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"total trainable parameters for raw model: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction_object):\n",
    "  preds = np.argmax(prediction_object.predictions, axis=1)\n",
    "  return {\"accuracy\": (preds == prediction_object.label_ids).mean()}\n",
    "\n",
    "def load_adapter_hyperparameters(model, params):\n",
    "    if adapter_type == \"bottleneck\":\n",
    "        adapter_config = BnConfig(mh_adapter=True,\n",
    "                                output_adapter=True,\n",
    "                                reduction_factor=params['bn_reduction_factor'],\n",
    "                                non_linearity='relu')\n",
    "    elif adapter_type == \"prefix\":\n",
    "        adapter_config = PrefixTuningConfig(prefix_length=params['prefix_length'])\n",
    "    elif adapter_type == \"prompt\":\n",
    "        adapter_config = PromptTuningConfig()\n",
    "\n",
    "    # Other adapter config options: \n",
    "    # ParBnConfig(reduction_factor=4)\n",
    "    # PrefixTuningConfig(flat=False, prefix_length=30)\n",
    "    # ConfigUnion(\n",
    "    #     PrefixTuningConfig(prefix_length=20),\n",
    "    #     ParBnConfig(reduction_factor=4),)\n",
    "\n",
    "    # Add a new adapter\n",
    "    default_name = \"default\"\n",
    "\n",
    "    model.delete_adapter(default_name)\n",
    "    model.add_adapter(default_name, config=adapter_config)\n",
    "\n",
    "    # Add a matching prediction head\n",
    "    if task_config.class_type == \"R\":\n",
    "        model.add_regression_head(default_name,\n",
    "                                overwrite_ok=True)\n",
    "    else:\n",
    "        model.add_classification_head(\n",
    "            default_name,\n",
    "            num_labels=task_config.num_classes,\n",
    "            overwrite_ok=True)\n",
    "\n",
    "    # Freeze all weights in the model except for those of the adapter\n",
    "    model.train_adapter(default_name)\n",
    "\n",
    "    # Activate the adapter\n",
    "    model.set_active_adapters(default_name)\n",
    "\n",
    "    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # print(f\"total trainable parameters for fine-tuning method: {num_trainable_params}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        num_train_epochs=params['num_train_epochs'],\n",
    "        per_device_train_batch_size=params['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=params['per_device_eval_batch_size'],\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        remove_unused_columns=False) # Ensures dataset labels are properly passed to the model\n",
    "\n",
    "    trainer = AdapterTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data[\"train\"],\n",
    "        eval_dataset=data[\"validation\"],\n",
    "        compute_metrics=compute_accuracy,\n",
    "        data_collator=default_data_collator)\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for saving results\n",
    "results_folder = pathlib.Path(f\"../../fast-results/results/{adapter_type}/{task_param}\")\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "save_file_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_folder / f\"val_{save_file_id}.csv\"\n",
    "display_best = float(\"-inf\")\n",
    "\n",
    "with open(results_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    header = [\"eval_accuracy\", \"training_time\", \"eval_time\"] + list(all_params[0].keys())\n",
    "    writer.writerow(header)\n",
    "\n",
    "bar = tqdm(enumerate(all_params), total=len(all_params))\n",
    "for i, params in bar:\n",
    "    # print(params) # verbose logging\n",
    "    trainer = load_adapter_hyperparameters(model, params)\n",
    "    train_stats = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "\n",
    "    display_recent = eval_result['eval_accuracy']\n",
    "    display_best = max(display_best, display_recent)\n",
    "\n",
    "    bar.set_description(f\"Best: {display_best:.5f}, Last: {display_recent:.5f}\")\n",
    "\n",
    "    training_time_per_epoch = train_stats.metrics['train_runtime'] / params['num_train_epochs']\n",
    "    eval_accuracy = eval_result['eval_accuracy']\n",
    "    eval_time_per_epoch = eval_result['eval_runtime']\n",
    "    # Write to results csv\n",
    "    with open(results_file, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        row = [eval_accuracy, training_time_per_epoch, eval_time_per_epoch] + list(params.values())\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(data[\"test\"])\n",
    "\n",
    "test_y_pred_file = results_folder / f\"y_pred_{save_file_id}.tsv\"\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=[\"prediction\"])\n",
    "y_pred_df[\"index\"] = y_pred_df.index\n",
    "y_pred_df = y_pred_df[[\"index\", \"prediction\"]]\n",
    "y_pred_df.to_csv(test_y_pred_file, sep='\\t', index=False, header=True)\n",
    "print(f\"saving predictions to ./{test_y_pred_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
