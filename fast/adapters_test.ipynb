{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Need: pip install accelerate -U\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from adapters import AutoAdapterModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = \"cpu\"  # default device is CPU\n",
    "if torch.cuda.is_available():\n",
    "    # I read that this works for detecting if notebook is being run in a colab environment, not sure though\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        print(\"colab environment\")\n",
    "        device_name = \"gpu\" \n",
    "    else:\n",
    "        device_name = \"cuda:0\" # CUDA for NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
    "\n",
    "# device_name = \"cuda:0\"\n",
    "device = torch.device(device_name)\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_param = \"cola\"\n",
    "TaskConfig = namedtuple(\"TaskConfig\", [\"sentence_type\", \"class_type\", \"num_classes\", \"col_names\"])\n",
    "\n",
    "task_configs = {\n",
    "    \"cola\": TaskConfig(\"one\", \"BC\", 1, ['sentence']),\n",
    "    \"sst2\": TaskConfig(\"one\", \"BC\", 1, ['sentence']),\n",
    "    \"mrpc\": TaskConfig(\"two\", \"BC\", 1, ['sentence1', 'sentence2']),\n",
    "    \"stsb\": TaskConfig(\"two\", \"R\", 1, ['sentence1', 'sentence2']),\n",
    "    \"qqp\": TaskConfig(\"two\", \"BC\", 1, ['question1', 'question2']),\n",
    "    \"mnli_matched\": TaskConfig(\"two\", \"MC\", 3, ['premise', 'hypothesis']),\n",
    "    \"mnli_mismatched\": TaskConfig(\"two\", \"MC\", 3, ['premise', 'hypothesis']),\n",
    "    \"qnli\": TaskConfig(\"two\", \"BC\", 1, ['question', 'sentence']),\n",
    "    \"rte\": TaskConfig(\"two\", \"BC\", 1, ['sentence1', 'sentence2']),\n",
    "    \"wnli\": TaskConfig(\"two\", \"BC\", 1, ['sentence1', 'sentence2']),\n",
    "}\n",
    "\n",
    "task_config = task_configs[task_param]\n",
    "\n",
    "if task_param == \"mnli_matched\": \n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_matched\"\n",
    "    test_key = \"test_matched\"\n",
    "elif task_param == \"mnli_mismatched\":\n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_mismatched\"\n",
    "    test_key = \"test_mismatched\"\n",
    "else:\n",
    "    data = load_dataset(\"glue\", task_param)\n",
    "    val_key = \"validation\"\n",
    "    test_key = \"test\"\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "max_len = 80 #512 # TODO: how was this decided?\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[task_config.col_names[0]],\n",
    "                     add_special_tokens=True, \n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=max_len,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "def tokenize_double(examples):\n",
    "    return tokenizer(examples[task_config.col_names[0]],\n",
    "                     examples[task_config.col_names[1]],\n",
    "                     add_special_tokens=True,\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=max_len,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "# Encode the input data\n",
    "data = data.map(tokenize, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "data = data.rename_column(original_column_name=\"label\", new_column_name=\"labels\") \n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "from adapters import AutoAdapterModel\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"distilroberta-base\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"distilroberta-base\",\n",
    "    config=config,\n",
    ")\n",
    "# model = AutoAdapterModel.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter setup\n",
    "from adapters import ConfigUnion, PrefixTuningConfig, ParBnConfig\n",
    "\n",
    "adapter_config = PrefixTuningConfig(flat=False, prefix_length=30)\n",
    "# # ConfigUnion(\n",
    "# #     PrefixTuningConfig(prefix_length=20),\n",
    "# #     ParBnConfig(reduction_factor=4),)\n",
    "\n",
    "# Add a new adapter\n",
    "model.add_adapter(task_param, config=adapter_config)\n",
    "\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    task_param,\n",
    "    num_labels=2\n",
    "  )\n",
    "\n",
    "# Activate the adapter\n",
    "model.train_adapter(task_param)\n",
    "model.set_active_adapters(task_param) # Possibly redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from adapters import AdapterTrainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=3e-4,\n",
    "#     max_steps=10000,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     logging_steps=1000,\n",
    "#     output_dir=\"adapter-roberta-base-amazon-polarity\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     remove_unused_columns=False,\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"adapter-roberta-base-amazon-polarity\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False, # Ensures dataset labels are properly passed to the model\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    "    data_collator=default_data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
