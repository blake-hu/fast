{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST: Feedforward-Augmented Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for running GLUE tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import pathlib\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils.feed_forward import FeedForward\n",
    "from utils.cls import extract_cls_embeddings\n",
    "from utils.mean_pooling import mean_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized default seed\n",
    "seed = 7\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"  # default device is CPU\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"  # CUDA for NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
    "device = torch.device(device_name)\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to set:\n",
    "- Model\n",
    "    - MPNetBase\n",
    "    - DistilRoBERTaBase\n",
    "    - MPNetST\n",
    "    - DistilRoBERTaST\n",
    "- Task\n",
    "    - cola\n",
    "    - sst2\n",
    "    - mrpc\n",
    "    - stsb\n",
    "    - qqp\n",
    "    - mnli-m\n",
    "    - mnli-mm\n",
    "    - qnli\n",
    "    - rte\n",
    "    - wnli\n",
    "- Embedding type\n",
    "    - cls\n",
    "    - mean_pooling\n",
    "    - sentence_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = \"DistilRoBERTaBase\"\n",
    "task_param = \"cola\"\n",
    "embedding_param = \"cls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if model_param == \"MPNetBase\": # MPNet Base\n",
    "    from transformers import MPNetTokenizer, MPNetModel\n",
    "    tokenizer = MPNetTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\").to(device)\n",
    "elif model_param == \"DistilRoBERTaBase\": # DistilRoBERTa Base\n",
    "    from transformers import RobertaTokenizer, RobertaModel\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "    model = RobertaModel.from_pretrained('distilroberta-base').to(device)\n",
    "elif model_param == \"MPNetST\": # MPNet Sentence Transformer\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "elif model_param == \"DistilRoBERTaST\": # DistilRoBERTa Sentence Transformer\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-distilroberta-v1').to(device)\n",
    "else:\n",
    "    raise Exception(f\"ERROR: Bad model_param\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_type: [\"one\", \"two\"]\n",
    "# class_type: [\"binary\", \"multi\", \"regression\"]\n",
    "# input_size: int (represents input size of feedforward, aka embedding size)\n",
    "# col_names: column names of relavent sentences on hugging face\n",
    "\n",
    "TaskConfig = namedtuple(\"TaskConfig\", [\"sentence_type\", \"class_type\", \"input_size\", \"col_names\"])\n",
    "task_configs = {\n",
    "    \"cola\": TaskConfig(\"one\", \"binary\", 768, ['sentence']),\n",
    "    \"sst2\": TaskConfig(\"one\", \"binary\", 768, ['sentence']),\n",
    "    \"mrpc\": TaskConfig(\"two\", \"binary\", 768*2, ['sentence1', 'sentence2']),\n",
    "    \"stsb\": TaskConfig(\"two\", \"regression\", 768*2, ['sentence1', 'sentence2']),\n",
    "    \"qqp\": TaskConfig(\"two\", \"binary\", 768*2, ['question1', 'question2']),\n",
    "    \"mnli-m\": TaskConfig(\"two\", \"multi\", 768*2, ['premise', 'hypothesis']),\n",
    "    \"mnli-mm\": TaskConfig(\"two\", \"multi\", 768*2, ['premise', 'hypothesis']),\n",
    "    \"qnli\": TaskConfig(\"two\", \"binary\", 768*2, ['question', 'sentence']),\n",
    "    \"rte\": TaskConfig(\"two\", \"binary\", 768*2, ['sentence1', 'sentence2']),\n",
    "    \"wnli\": TaskConfig(\"two\", \"binary\", 768*2, ['sentence1', 'sentence2']),\n",
    "}\n",
    "\n",
    "task_config = task_configs[task_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"glue\", task_param)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUESingleSentence(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GLUESingleSentence(data['train']['sentence'], tokenizer)\n",
    "val_dataset = GLUESingleSentence(data['validation']['sentence'], tokenizer)\n",
    "test_dataset = GLUESingleSentence(data['test']['sentence'], tokenizer)\n",
    "\n",
    "# pick batch size based on GPU memory\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:30<00:00,  1.77s/it]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.70s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8551, 768])\n",
      "torch.Size([1043, 768])\n",
      "torch.Size([1063, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    masked_embeddings = token_embeddings * input_mask_expanded\n",
    "    mean_embeddings = torch.sum(masked_embeddings, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return torch.nn.functional.normalize(mean_embeddings, p=2, dim=1)\n",
    "\n",
    "def extract_cls(model_output):\n",
    "    last_hidden_state = model_output['last_hidden_state']\n",
    "    cls_embedding = last_hidden_state[:, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "def compute_embeddings(loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        for input_ids, attention_mask in tqdm(loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # embeding = mean_pooling(outputs, attention_mask)\n",
    "            embeding = extract_cls(outputs)\n",
    "\n",
    "            embeddings.append(embeding)\n",
    "\n",
    "        return torch.cat(embeddings, 0)\n",
    "    \n",
    "train_embed = compute_embeddings(train_loader)\n",
    "val_embed = compute_embeddings(val_loader)\n",
    "test_embed = compute_embeddings(test_loader)\n",
    "\n",
    "print(train_embed.shape)\n",
    "print(val_embed.shape)\n",
    "print(test_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_embed.cpu().numpy()\n",
    "X_val = val_embed.cpu().numpy()\n",
    "X_test = test_embed.cpu().numpy()\n",
    "\n",
    "Y_train = np.array(data[\"train\"][\"label\"])\n",
    "Y_val = np.array(data[\"validation\"][\"label\"])\n",
    "Y_test = np.array(data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### previous system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~13 minutes for CLS\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, sentences2=None):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.sentences2 = sentences2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if embedding_param == \"cls\" or embedding_param == \"mean_pooling\":\n",
    "            # Tokenize the sentence\n",
    "            tokenized = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "            # Perform model inference\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokenized)\n",
    "        elif embedding_param == \"sentence_transformer\":\n",
    "            with torch.no_grad():\n",
    "                sentence2 = self.sentences2[idx]\n",
    "                embeddings = model.encode(sentence) # j gonna call them embeddings for now x\n",
    "                if task_config.sentence_type == \"two\":\n",
    "                    tokenized_2 = model.encode(sentence2)\n",
    "                    embeddings = np.concatenate([embeddings, tokenized_2], axis=1)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        if embedding_param == \"cls\":\n",
    "            embeddings = extract_cls_embeddings(outputs)  \n",
    "        elif embedding_param == \"mean_pooling\":\n",
    "            embeddings = mean_pooling(outputs, outputs['attention_mask'])\n",
    "\n",
    "        return embeddings, label\n",
    "\n",
    "\n",
    "if embedding_param == \"sentence_transformer\" and task_config.sentence_type == \"two\":\n",
    "    train_dataset = CustomDataset(data[\"train\"][task_config.col_names[0]], data[\"train\"][\"label\"], data[\"train\"][task_config.col_names[1]])\n",
    "    val_dataset = CustomDataset(data[\"validation\"][task_config.col_names[0]], data[\"validation\"][\"label\"], data[\"validation\"][task_config.col_names[1]])\n",
    "    test_dataset = CustomDataset(data[\"test\"][task_config.col_names[0]], data[\"test\"][\"label\"], data[\"test\"][task_config.col_names[1]])\n",
    "else: # if cls, mp, or st type one\n",
    "    train_dataset = CustomDataset(data[\"train\"][task_config.col_names[0]], data[\"train\"][\"label\"])\n",
    "    val_dataset = CustomDataset(data[\"validation\"][task_config.col_names[0]], data[\"validation\"][\"label\"])\n",
    "    test_dataset = CustomDataset(data[\"test\"][task_config.col_names[0]], data[\"test\"][\"label\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "# DataLoader iteration\n",
    "for batch in train_dataloader:\n",
    "    X_train, Y_train = batch\n",
    "    # X_train = np.array(X_train) # When you iterate through a DataLoader, the batch that is returned from the dataset is automatically converted into tensors by PyTorch. Convert BACK?!\n",
    "for batch in val_dataloader:\n",
    "    X_val, Y_val = batch\n",
    "#     X_val = np.array(X_val) \n",
    "for batch in test_dataloader:\n",
    "    X_test, Y_test = batch\n",
    "#     X_test = np.array(X_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving embeddings to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "# TODO: load_file has bugs :(\n",
    "output_directory = f\"./output/{embedding_param}\"\n",
    "\n",
    "output_path = pathlib.Path(output_directory)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if embedding_param == \"cls\" or embedding_param == \"mean_pooling\":\n",
    "    x_suffix = \"npy\"\n",
    "    def save_file(file1, file2):\n",
    "        np.save(file1, file2)\n",
    "    def load_file(file1):\n",
    "        np.load(file1, allow_pickle=True)\n",
    "elif embedding_param == \"sentence_transformer\":\n",
    "    x_suffix = \"pt\"\n",
    "    def save_file(file1, file2):\n",
    "        torch.save(file1, file2)\n",
    "    def load_file(file1):\n",
    "        torch.load(file1)\n",
    "\n",
    "# write\n",
    "with open(f'{output_directory}/X_train_{task_param}_{model_param}.{x_suffix}', 'wb') as X_train_file:\n",
    "    save_file(X_train_file, X_train)\n",
    "with open(f'{output_directory}/X_val_{task_param}_{model_param}.{x_suffix}', 'wb') as X_val_file:\n",
    "    save_file(X_val_file, X_val)\n",
    "with open(f'{output_directory}/X_test_{task_param}_{model_param}.{x_suffix}', 'wb') as X_test_file:\n",
    "    save_file(X_test_file, X_test)\n",
    "with open(f'{output_directory}/Y_train_{task_param}_{model_param}.npy', 'wb') as Y_train_file:\n",
    "    np.save(Y_train_file, Y_train)\n",
    "with open(f'{output_directory}/Y_val_{task_param}_{model_param}.npy', 'wb') as Y_val_file:\n",
    "    np.save(Y_val_file, Y_val)\n",
    "with open(f'{output_directory}/Y_test_{task_param}_{model_param}.npy', 'wb') as Y_test_file:\n",
    "    np.save(Y_test_file, Y_test)\n",
    "\n",
    "# read\n",
    "with open(f'{output_directory}/X_train_{task_param}_{model_param}.{x_suffix}', 'rb') as X_train_file:\n",
    "    X_train = load_file(X_train_file)\n",
    "with open(f'{output_directory}/X_val_{task_param}_{model_param}.{x_suffix}', 'rb') as X_val_file:\n",
    "    X_val = load_file(X_val_file)\n",
    "with open(f'{output_directory}/X_test_{task_param}_{model_param}.{x_suffix}', 'rb') as X_test_file:\n",
    "    X_test = load_file(X_test_file)\n",
    "with open(f'{output_directory}/Y_train_{task_param}_{model_param}.npy', 'rb') as Y_train_file:\n",
    "    Y_train = np.load(Y_train_file)\n",
    "with open(f'{output_directory}/Y_val_{task_param}_{model_param}.npy', 'rb') as Y_val_file:\n",
    "    Y_val = np.load(Y_val_file)\n",
    "with open(f'{output_directory}/Y_test_{task_param}_{model_param}.npy', 'rb') as Y_test_file:\n",
    "    Y_test = np.load(Y_test_file)\n",
    "\n",
    "print(f\"size of X_train: {len(X_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 hyperparameter combinations\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'num_epochs': [50],\n",
    "    'batch_size': [32, 128, 512],\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'category': ['BC'],\n",
    "    'norm': [False],\n",
    "    'input_size': [task_config.input_size],\n",
    "    'layer_size': [task_config.input_size],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'weight_decay':[1e-2, 1e-3, 1e-4],\n",
    "    'patience': [3],\n",
    "    'min_delta': [0],\n",
    "    'device': [device_name]\n",
    "}\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "print(f\"{len(all_params)} hyperparameter combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results to ./output/val_results_cls_cola_20231129_140656.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best Acc: 0.76414, Last test: 0.75935: 100%|██████████| 54/54 [01:59<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# setup for logging\n",
    "save_file_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "console_output_filename = f'./output/{task_param}_console_output.txt'\n",
    "with open(console_output_filename, 'a') as logfile:\n",
    "    logfile.write('\\n\\nBEGIN TRAINING LOOP\\n\\n')\n",
    "results_filename = f'./output/val_results_{embedding_param}_{task_param}_{save_file_id}.csv'\n",
    "with open(results_filename, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    headers = list(all_params[0].keys())\n",
    "    writer.writerow(['mcc', 'f1', 'accuracy'] + headers)\n",
    "print(f\"saving results to {results_filename}\")\n",
    "\n",
    "# saves best accuracy for progress bar display\n",
    "best_acc = 0\n",
    "# Iterate over all combinations of hyperparameters\n",
    "bar = tqdm(enumerate(all_params), total=len(all_params))\n",
    "for i, params in bar:\n",
    "    # formatting params to display\n",
    "    print_params = params.copy()\n",
    "    for param in ['category', 'device']:\n",
    "        del print_params[param]\n",
    "    \n",
    "    # Initialize the model with current set of hyperparameters\n",
    "    feed_forward = FeedForward(**params)\n",
    "\n",
    "    # Print stats to console\n",
    "    epoch, val_loss, val_accuracy, val_f1, val_mcc = feed_forward.fit(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "    best_acc = max(best_acc, val_accuracy)\n",
    "    bar.set_description(f\"Best Acc: {best_acc:.5f}, Last test: {val_accuracy:.5f}\")\n",
    "\n",
    "    # Write stats to log file\n",
    "    with open(console_output_filename, 'a') as logfile:\n",
    "        logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
    "        logfile.write(f\"\\nEarly stopped on epoch: {epoch}\")\n",
    "        logfile.write(f\"\\nValidation accuracy: {val_accuracy}\")\n",
    "        logfile.write(f\"\\nValidation f1-score: {val_f1}\")\n",
    "        logfile.write(f\"\\nValidation MCC     : {val_mcc}\")\n",
    "\n",
    "    with open(results_filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([val_mcc, val_f1, val_accuracy] + list(params.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mcc: 0.43688\n",
      "        mcc        f1  accuracy  num_epochs  batch_size  learning_rate  \\\n",
      "5  0.436881  0.758389  0.758389          50          32           0.01   \n",
      "\n",
      "  category   norm  input_size  layer_size  num_layers  weight_decay  patience  \\\n",
      "5       BC  False         768         768           2        0.0001         3   \n",
      "\n",
      "   min_delta device  \n",
      "5          0   cuda  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(results_filename)\n",
    "# results_df = pd.read_csv(\"output/val_results_cola_20231127_151717.csv\")\n",
    "metric = \"mcc\"\n",
    "best = results_df[metric].max()\n",
    "best_row = results_df[results_df[metric] == best]\n",
    "print(f\"Best {metric}: {best:.5f}\")\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set this based on output CSV file\n",
    "best_params = {\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-2,\n",
    "    'category': 'C',\n",
    "    'norm': False,\n",
    "    'input_size': 768,\n",
    "    'layer_size': 6,\n",
    "    'num_layers': 3,\n",
    "    'weight_decay':1e-2,\n",
    "    'patience': 3,\n",
    "    'min_delta': 0,\n",
    "    'device': device_name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feed_forward = FeedForward(**best_params)\n",
    "\n",
    "X = np.concatenate((X_train, X_val), axis=0)\n",
    "Y = np.concatenate((Y_train, Y_val), axis=0)\n",
    "\n",
    "best_feed_forward.fit(X, Y)\n",
    "\n",
    "preds = np.argmax(best_feed_forward.predict_proba(X_test), axis=1)\n",
    "print(preds.shape)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'index': range(len(preds)),\n",
    "    'prediction': preds\n",
    "})\n",
    "\n",
    "random.seed()  # set random seed based on current time just to generate random file_id\n",
    "random_file_id = str(round(random.random() * 10000))\n",
    "random.seed(0)  # reset random seed back to standard 0 seed\n",
    "# Write the DataFrame to a .tsv file, without the header and index\n",
    "df.to_csv(f'CoLA_{random_file_id}.tsv', sep='\\t', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
