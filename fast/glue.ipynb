{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FAST: Feedforward-Augmented Sentence Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notebook for running GLUE tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import csv\n",
        "import pathlib\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from collections import namedtuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "from utils.feed_forward import FeedForward\n",
        "from utils.cls import extract_cls_embeddings\n",
        "from utils.mean_pooling import mean_pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# standardized default seed\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "device_name = \"cpu\"  # default device is CPU\n",
        "if torch.cuda.is_available():\n",
        "    device_name = \"gpu\"  # CUDA for NVIDIA GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
        "\n",
        "device = torch.device(device_name)\n",
        "print(device_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## User parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters to set:\n",
        "- Model\n",
        "    - MPNetBase\n",
        "    - DistilRoBERTaBase\n",
        "    - MPNetST\n",
        "    - DistilRoBERTaST\n",
        "- Task\n",
        "    - cola\n",
        "    - sst2\n",
        "    - mrpc\n",
        "    - stsb\n",
        "    - qqp\n",
        "    - mnli_matched\n",
        "    - mnli_mismatched\n",
        "    - qnli\n",
        "    - rte\n",
        "    - wnli\n",
        "- Embedding type\n",
        "    - cls\n",
        "    - mean_pooling\n",
        "    - sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_param = \"DistilRoBERTaST\"\n",
        "task_param = \"stsb\"\n",
        "embedding_param = \"sentence\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_param == \"MPNetBase\": # MPNet Base\n",
        "    from transformers import MPNetTokenizer, MPNetModel\n",
        "    tokenizer = MPNetTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
        "    model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\").to(device)\n",
        "elif model_param == \"DistilRoBERTaBase\": # DistilRoBERTa Base\n",
        "    from transformers import RobertaTokenizer, RobertaModel\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
        "    model = RobertaModel.from_pretrained('distilroberta-base').to(device)\n",
        "elif model_param == \"MPNetST\": # MPNet Sentence Transformer\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
        "elif model_param == \"DistilRoBERTaST\": # DistilRoBERTa Sentence Transformer\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model = SentenceTransformer('sentence-transformers/all-distilroberta-v1').to(device)\n",
        "else:\n",
        "    raise Exception(f\"ERROR: Bad model_param\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sentence_type: [\"one\", \"two\"]\n",
        "# class_type: [\"BC\", \"MC\", \"R\"]\n",
        "# input_size: int (represents input size of feedforward, aka embedding size)\n",
        "# col_names: column names of relavent sentences on hugging face\n",
        "\n",
        "TaskConfig = namedtuple(\"TaskConfig\", [\"sentence_type\", \"class_type\", \"input_size\", \"col_names\"])\n",
        "task_configs = {\n",
        "    \"cola\": TaskConfig(\"one\", \"BC\", 768, ['sentence']),\n",
        "    \"sst2\": TaskConfig(\"one\", \"BC\", 768, ['sentence']),\n",
        "    \"mrpc\": TaskConfig(\"two\", \"BC\", 768*2, ['sentence1', 'sentence2']),\n",
        "    \"stsb\": TaskConfig(\"two\", \"R\", 768*2, ['sentence1', 'sentence2']),\n",
        "    \"qqp\": TaskConfig(\"two\", \"BC\", 768*2, ['question1', 'question2']),\n",
        "    \"mnli_matched\": TaskConfig(\"two\", \"MC\", 768*2, ['premise', 'hypothesis']),\n",
        "    \"mnli_mismatched\": TaskConfig(\"two\", \"MC\", 768*2, ['premise', 'hypothesis']),\n",
        "    \"qnli\": TaskConfig(\"two\", \"BC\", 768*2, ['question', 'sentence']),\n",
        "    \"rte\": TaskConfig(\"two\", \"BC\", 768*2, ['sentence1', 'sentence2']),\n",
        "    \"wnli\": TaskConfig(\"two\", \"BC\", 768*2, ['sentence1', 'sentence2']),\n",
        "}\n",
        "\n",
        "task_config = task_configs[task_param]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 5749\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1379\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if task_param == \"mnli_matched\": \n",
        "    data = load_dataset(\"glue\", \"mnli\") \n",
        "    val_key = \"validation_matched\"\n",
        "    test_key = \"test_matched\"\n",
        "elif task_param == \"mnli_mismatched\":\n",
        "    data = load_dataset(\"glue\", \"mnli\") \n",
        "    val_key = \"validation_mismatched\"\n",
        "    test_key = \"test_mismatched\"\n",
        "else:\n",
        "    data = load_dataset(\"glue\", task_param)\n",
        "    val_key = \"validation\"\n",
        "    test_key = \"test\"\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Labels come directly from dataset so no need to save to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y_train = np.array(data[\"train\"][\"label\"])\n",
        "Y_val = np.array(data[val_key][\"label\"])\n",
        "Y_test = np.array(data[test_key][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check for saved embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "cache_dir = f\"./cache/{embedding_param}/{task_param}\"\n",
        "cache_path = pathlib.Path(cache_dir)\n",
        "cache_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "file_names = ['X_train', 'X_val', 'X_test']\n",
        "paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
        "\n",
        "use_cached_embeddings = False\n",
        "if all(path.exists() for path in paths):\n",
        "    print(\"saved embeddings found!\")\n",
        "    X_train = np.load(paths[0])\n",
        "    X_val = np.load(paths[1])\n",
        "    X_test = np.load(paths[2])\n",
        "    use_cached_embeddings = True\n",
        "else:\n",
        "    print(\"no saved embeddings found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GLUESingleSentence(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text, \n",
        "            add_special_tokens=True, \n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)\n",
        "    \n",
        "class GLUEPairedSentence(Dataset):\n",
        "    def __init__(self, texts1, texts2, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts1 = texts1\n",
        "        self.texts2 = texts2\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text1 = self.texts1[idx]\n",
        "        text2 = self.texts2[idx]\n",
        "\n",
        "        inputs1 = self.tokenizer.encode_plus(\n",
        "            text1, \n",
        "            add_special_tokens=True, \n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        inputs2 = self.tokenizer.encode_plus(\n",
        "            text2, \n",
        "            add_special_tokens=True, \n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return inputs1['input_ids'].squeeze(0), inputs1['attention_mask'].squeeze(0), \\\n",
        "               inputs2['input_ids'].squeeze(0), inputs2['attention_mask'].squeeze(0)\n",
        "\n",
        "class GLUEPairedSentenceST(Dataset):\n",
        "    def __init__(self, texts1, texts2, max_length=512):\n",
        "        self.texts1 = texts1\n",
        "        self.texts2 = texts2\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs1 = self.texts1[idx]\n",
        "        inputs2 = self.texts2[idx]\n",
        "\n",
        "        return inputs1, inputs2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if task_config.sentence_type == \"one\":\n",
        "    train_dataset = GLUESingleSentence(data['train']['sentence'], tokenizer)\n",
        "    val_dataset = GLUESingleSentence(data['validation']['sentence'], tokenizer)\n",
        "    test_dataset = GLUESingleSentence(data['test']['sentence'], tokenizer)\n",
        "    \n",
        "elif task_config.sentence_type == \"two\":\n",
        "    key1 = task_config.col_names[0]\n",
        "    key2 = task_config.col_names[1]\n",
        "    \n",
        "    if embedding_param in [\"cls\", \"mean_pooling\"]:\n",
        "        train_dataset = GLUEPairedSentence(data['train'][key1], data['train'][key2], tokenizer)\n",
        "        val_dataset = GLUEPairedSentence(data['validation'][key1], data['validation'][key2], tokenizer)\n",
        "        test_dataset = GLUEPairedSentence(data['test'][key1], data['test'][key2], tokenizer)\n",
        "    elif embedding_param == \"sentence\":\n",
        "        train_dataset = GLUEPairedSentenceST(data['train'][key1], data['train'][key2])\n",
        "        val_dataset = GLUEPairedSentenceST(data['validation'][key1], data['validation'][key2])\n",
        "        test_dataset = GLUEPairedSentenceST(data['test'][key1], data['test'][key2])\n",
        "        \n",
        "else:\n",
        "    raise Exception(f\"{task_config.sentence_type}: sentence type not recognized\")\n",
        "\n",
        "# pick batch size based on GPU memory\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████| 360/360 [00:53<00:00,  6.69it/s]\n",
            "100%|████████████████████████████████████████| 94/94 [00:15<00:00,  6.25it/s]\n",
            "100%|████████████████████████████████████████| 87/87 [00:12<00:00,  6.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5749, 1536)\n",
            "(1500, 1536)\n",
            "(1379, 1536)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    masked_embeddings = token_embeddings * input_mask_expanded\n",
        "    mean_embeddings = torch.sum(masked_embeddings, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return torch.nn.functional.normalize(mean_embeddings, p=2, dim=1)\n",
        "\n",
        "def extract_cls_embeddings(model_output):\n",
        "    last_hidden_state = model_output['last_hidden_state']\n",
        "    cls_embedding = last_hidden_state[:, 0, :]\n",
        "    return cls_embedding\n",
        "\n",
        "def compute_single_embeddings(loader):\n",
        "    embeddings = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask in tqdm(loader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            if embedding_param == \"cls\":\n",
        "                embed = extract_cls_embeddings(outputs)\n",
        "            elif embedding_param == \"mean_pooling\":\n",
        "                embed = mean_pooling(outputs, attention_mask)\n",
        "                \n",
        "            embeddings.append(embed.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "def compute_pair_embeddings(loader):\n",
        "    embeddings = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        if embedding_param in [\"cls\", \"mean_pooling\"]:\n",
        "            for input_ids1, attention_mask1, input_ids2, attention_mask2 in tqdm(loader):\n",
        "                input_ids1 = input_ids1.to(device)\n",
        "                attention_mask1 = attention_mask1.to(device)\n",
        "                input_ids2 = input_ids2.to(device)\n",
        "                attention_mask2 = attention_mask2.to(device)\n",
        "    \n",
        "                outputs1 = model(input_ids1, attention_mask=attention_mask1)\n",
        "                outputs2 = model(input_ids2, attention_mask=attention_mask2)\n",
        "                    \n",
        "                if embedding_param == \"cls\":\n",
        "                    U = extract_cls_embeddings(outputs1)\n",
        "                    V = extract_cls_embeddings(outputs2)\n",
        "                elif embedding_param == \"mean_pooling\":\n",
        "                    U = mean_pooling(outputs1, attention_mask1)\n",
        "                    V = mean_pooling(outputs2, attention_mask2)\n",
        "                    \n",
        "                embed = torch.cat([U, V], dim=1)\n",
        "                embeddings.append(embed.cpu().numpy())\n",
        "        \n",
        "        elif embedding_param == \"sentence\":\n",
        "            for input_ids1, input_ids2 in tqdm(loader):\n",
        "                U = torch.tensor(model.encode(input_ids1))\n",
        "                V = torch.tensor(model.encode(input_ids2))\n",
        "                embed = torch.cat([U, V], dim=1)\n",
        "                embeddings.append(embed.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "if use_cached_embeddings == False:\n",
        "    if task_config.sentence_type == \"one\":\n",
        "        train_embed = compute_single_embeddings(train_loader)\n",
        "        val_embed = compute_single_embeddings(val_loader)\n",
        "        test_embed = compute_single_embeddings(test_loader)\n",
        "    elif task_config.sentence_type == \"two\":\n",
        "        train_embed = compute_pair_embeddings(train_loader)\n",
        "        val_embed = compute_pair_embeddings(val_loader)\n",
        "        test_embed = compute_pair_embeddings(test_loader)\n",
        "\n",
        "    print(train_embed.shape)\n",
        "    print(val_embed.shape)\n",
        "    print(test_embed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "if use_cached_embeddings == False:\n",
        "    X_train = train_embed\n",
        "    X_val = val_embed\n",
        "    X_test = test_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5749, 1536)\n",
            "(1500, 1536)\n",
            "(1379, 1536)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving embeddings to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "cache_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "file_names = ['X_train', 'X_val', 'X_test']\n",
        "paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
        "\n",
        "with open(paths[0], 'wb') as X_train_file:\n",
        "    np.save(X_train_file, X_train)\n",
        "with open(paths[1], 'wb') as X_val_file:\n",
        "    np.save(X_val_file, X_val)\n",
        "with open(paths[2], 'wb') as X_test_file:\n",
        "    np.save(X_test_file, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72 hyperparameter combinations\n"
          ]
        }
      ],
      "source": [
        "input_size = task_config.input_size\n",
        "\n",
        "param_grid = {\n",
        "    'num_epochs': [50],\n",
        "    'batch_size': [32, 512],\n",
        "    'learning_rate': [1e-2, 1e-3],\n",
        "    'category': [task_config.class_type],\n",
        "    'norm': [False],\n",
        "    'input_size': [input_size],\n",
        "    'layer_size': [input_size // 2, input_size, input_size * 2],\n",
        "    'num_layers': [1, 5, 10],\n",
        "    'weight_decay':[1e-2, 1e-4],\n",
        "    'patience': [3],\n",
        "    'min_delta': [0],\n",
        "    'device': [device_name]\n",
        "}\n",
        "\n",
        "# Create a list of all combinations of hyperparameters\n",
        "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
        "print(f\"{len(all_params)} hyperparameter combinations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saving results to ./results/sentence/stsb/val_20231201_143434_DistilRoBERTaST.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Best Pearson's corrcoef: 0.590, Last test: 0.289:   7%| | 5/72 [00:57<13:03, /Users/blake/miniforge3/envs/fast/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/Users/blake/miniforge3/envs/fast/lib/python3.9/site-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "Best Pearson's corrcoef: 0.648, Last test: 0.488: 100%|█| 72/72 [13:55<00:00,\n"
          ]
        }
      ],
      "source": [
        "from utils.feed_forward import FeedForward\n",
        "\n",
        "# setup for logging\n",
        "console_output_filename = f'./output/{embedding_param}_{task_param}_console_output.txt'\n",
        "with open(console_output_filename, 'a') as logfile:\n",
        "    logfile.write('\\n\\nBEGIN TRAINING LOOP\\n\\n')\n",
        "# setup for saving results\n",
        "results_folder = pathlib.Path(f\"results/{embedding_param}/{task_param}\")\n",
        "results_folder.mkdir(parents=True, exist_ok=True)\n",
        "save_file_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_file = results_folder / f\"val_{save_file_id}_{model_param}.csv\"\n",
        "\n",
        "if task_config.class_type in [\"BC\", \"MC\"]:\n",
        "    with open(results_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        headers = list(all_params[0].keys())\n",
        "        writer.writerow(['mcc', 'f1', 'accuracy'] + headers)\n",
        "    print(f\"saving results to ./{results_file}\")\n",
        "    # saves best accuracy for progress bar display\n",
        "    best_acc = 0.0\n",
        "elif task_config.class_type == \"R\":\n",
        "    with open(results_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        headers = list(all_params[0].keys())\n",
        "        writer.writerow(['pearson', 'spearman'] + headers)\n",
        "    print(f\"saving results to ./{results_file}\")\n",
        "    # saves best accuracy for progress bar display\n",
        "    best_pearson = -2.0\n",
        "    \n",
        "# Iterate over all combinations of hyperparameters\n",
        "bar = tqdm(enumerate(all_params), total=len(all_params))\n",
        "for i, params in bar:\n",
        "    # formatting params to display\n",
        "    print_params = params.copy()\n",
        "    for param in ['category', 'device']:\n",
        "        del print_params[param]\n",
        "    \n",
        "    # Initialize the model with current set of hyperparameters\n",
        "    feed_forward = FeedForward(**params)\n",
        "\n",
        "    metrics = feed_forward.fit(X_train, Y_train, X_val, Y_val)\n",
        "\n",
        "    if task_config.class_type in [\"BC\", \"MC\"]:\n",
        "        epoch, val_loss, val_accuracy, val_f1, val_mcc = metrics[\"epoch\"], metrics[\"loss\"], metrics[\"acc\"], metrics[\"f1\"], metrics[\"mcc\"]\n",
        "        best_acc = max(best_acc, val_accuracy)\n",
        "        bar.set_description(f\"Best Acc: {best_acc:.5f}, Last test: {val_accuracy:.5f}\")\n",
        "    \n",
        "        # Write stats to log file\n",
        "        with open(console_output_filename, 'a') as logfile:\n",
        "            logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
        "            logfile.write(f\"\\nEarly stopped on epoch: {epoch}\")\n",
        "            logfile.write(f\"\\nValidation accuracy: {val_accuracy}\")\n",
        "            logfile.write(f\"\\nValidation f1-score: {val_f1}\")\n",
        "            logfile.write(f\"\\nValidation MCC     : {val_mcc}\")\n",
        "        # write to results csv\n",
        "        with open(results_file, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([val_mcc, val_f1, val_accuracy] + list(params.values()))\n",
        "            \n",
        "    elif task_config.class_type == \"R\":  # just report loss for regression task\n",
        "        epoch, val_loss, val_pearson, val_spearman = metrics[\"epoch\"], metrics[\"loss\"], metrics[\"pearson\"], metrics[\"spearman\"]\n",
        "        best_pearson = max(best_pearson, val_pearson)\n",
        "        bar.set_description(f\"Best Pearson's corrcoef: {best_pearson:.3f}, Last test: {val_pearson:.3f}\")\n",
        "    \n",
        "        # Write stats to log file\n",
        "        with open(console_output_filename, 'a') as logfile:\n",
        "            logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
        "            logfile.write(f\"\\nEarly stopped on epoch: {epoch}\")\n",
        "            logfile.write(f\"\\nValidation loss: {val_loss}\")\n",
        "            logfile.write(f\"\\nValidation Pearson's corrcoef: {val_pearson}\")\n",
        "            logfile.write(f\"\\nValidation Spearman's corrcoef: {val_spearman}\")\n",
        "        # write to results csv\n",
        "        with open(results_file, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([val_pearson, val_spearman] + list(params.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best pearson: 0.64803\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pearson</th>\n",
              "      <th>spearman</th>\n",
              "      <th>num_epochs</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>category</th>\n",
              "      <th>norm</th>\n",
              "      <th>input_size</th>\n",
              "      <th>layer_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>weight_decay</th>\n",
              "      <th>patience</th>\n",
              "      <th>min_delta</th>\n",
              "      <th>device</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.648031</td>\n",
              "      <td>0.639333</td>\n",
              "      <td>50</td>\n",
              "      <td>32</td>\n",
              "      <td>0.001</td>\n",
              "      <td>R</td>\n",
              "      <td>False</td>\n",
              "      <td>1536</td>\n",
              "      <td>1536</td>\n",
              "      <td>10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>mps</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pearson  spearman  num_epochs  batch_size  learning_rate category   norm  \\\n",
              "28  0.648031  0.639333          50          32          0.001        R  False   \n",
              "\n",
              "    input_size  layer_size  num_layers  weight_decay  patience  min_delta  \\\n",
              "28        1536        1536          10          0.01         3          0   \n",
              "\n",
              "   device  \n",
              "28    mps  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df = pd.read_csv(results_file)\n",
        "# results_df = pd.read_csv(\"output/val_results_cola_20231127_151717.csv\")\n",
        "if task_config.class_type in [\"BC\", \"MC\"]:\n",
        "    metric = \"accuracy\"\n",
        "    best = results_df[metric].max()\n",
        "    best_row = results_df[results_df[metric] == best]\n",
        "elif task_config.class_type == \"R\":\n",
        "    metric = \"pearson\"\n",
        "    best = results_df[metric].max()\n",
        "    best_row = results_df[results_df[metric] == best]\n",
        "    \n",
        "print(f\"Best {metric}: {best:.5f}\")\n",
        "best_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
