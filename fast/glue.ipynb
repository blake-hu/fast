{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST: Feedforward-Augmented Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for running GLUE tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import pathlib\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils.feed_forward import FeedForward\n",
    "from utils.cls import extract_cls_embeddings\n",
    "from utils.mean_pooling import mean_pooling\n",
    "from utils.energy import get_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized default seed\n",
    "seed = 7\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"  # default device is CPU\n",
    "if torch.cuda.is_available():\n",
    "    # I read that this works for detecting if notebook is being run in a colab environment, not sure though\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        print(\"I'm running on Colab\")\n",
    "        device_name = \"gpu\" \n",
    "    else:\n",
    "        device_name = \"cuda\" # CUDA for NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to set:\n",
    "- Model\n",
    "    - MPNetBase\n",
    "    - DistilRoBERTaBase\n",
    "    - MPNetST\n",
    "    - DistilRoBERTaST\n",
    "- Task\n",
    "    - cola\n",
    "    - sst2\n",
    "    - mrpc\n",
    "    - stsb\n",
    "    - qqp\n",
    "    - mnli_matched\n",
    "    - mnli_mismatched\n",
    "    - qnli\n",
    "    - rte\n",
    "    - wnli\n",
    "- Embedding type\n",
    "    - cls\n",
    "    - mean_pooling\n",
    "    - sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = \"DistilRoBERTaBase\"\n",
    "task_param = \"wnli\"\n",
    "embedding_param = \"mean_pooling\"\n",
    "\n",
    "use_carbontracker = False \n",
    "# NOTE: Carbontracker should generate its own output file \n",
    "# => logged benchmarking results will be dummy values if use_carbontracker = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if model_param == \"MPNetBase\": # MPNet Base\n",
    "    from transformers import MPNetTokenizer, MPNetModel\n",
    "    tokenizer = MPNetTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\").to(device)\n",
    "elif model_param == \"DistilRoBERTaBase\": # DistilRoBERTa Base\n",
    "    from transformers import RobertaTokenizer, RobertaModel\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "    model = RobertaModel.from_pretrained('distilroberta-base').to(device)\n",
    "elif model_param == \"MPNetST\": # MPNet Sentence Transformer\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "elif model_param == \"DistilRoBERTaST\": # DistilRoBERTa Sentence Transformer\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-distilroberta-v1').to(device)\n",
    "else:\n",
    "    raise Exception(f\"ERROR: Bad model_param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_type: [\"one\", \"two\"]\n",
    "# class_type: [\"BC\", \"MC\", \"R\"]\n",
    "# input_size: int (represents input size of feedforward, aka embedding size)\n",
    "# col_names: column names of relavent sentences on hugging face\n",
    "# num_classes needs to be 1 for BC and R, only change value to number of classes for MC\n",
    "\n",
    "TaskConfig = namedtuple(\"TaskConfig\", [\"sentence_type\", \"class_type\", \"num_classes\", \"input_size\", \"col_names\"])\n",
    "\n",
    "task_configs = {\n",
    "    \"cola\": TaskConfig(\"one\", \"BC\", 1, 768, ['sentence']),\n",
    "    \"sst2\": TaskConfig(\"one\", \"BC\", 1, 768, ['sentence']),\n",
    "    \"mrpc\": TaskConfig(\"two\", \"BC\", 1, 768*2, ['sentence1', 'sentence2']),\n",
    "    \"stsb\": TaskConfig(\"two\", \"R\", 1, 768*2, ['sentence1', 'sentence2']),\n",
    "    \"qqp\": TaskConfig(\"two\", \"BC\", 1, 768*2, ['question1', 'question2']),\n",
    "    \"mnli_matched\": TaskConfig(\"two\", \"MC\", 3, 768*2, ['premise', 'hypothesis']),\n",
    "    \"mnli_mismatched\": TaskConfig(\"two\", \"MC\", 3, 768*2, ['premise', 'hypothesis']),\n",
    "    \"qnli\": TaskConfig(\"two\", \"BC\", 1, 768*2, ['question', 'sentence']),\n",
    "    \"rte\": TaskConfig(\"two\", \"BC\", 1, 768*2, ['sentence1', 'sentence2']),\n",
    "    \"wnli\": TaskConfig(\"two\", \"BC\", 1, 768*2, ['sentence1', 'sentence2']),\n",
    "}\n",
    "\n",
    "task_config = task_configs[task_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 635\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 71\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 146\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if task_param == \"mnli_matched\": \n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_matched\"\n",
    "    test_key = \"test_matched\"\n",
    "elif task_param == \"mnli_mismatched\":\n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_mismatched\"\n",
    "    test_key = \"test_mismatched\"\n",
    "else:\n",
    "    data = load_dataset(\"glue\", task_param)\n",
    "    val_key = \"validation\"\n",
    "    test_key = \"test\"\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels come directly from dataset so no need to save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635,)\n",
      "(71,)\n",
      "(146,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Y_train = np.array(data[\"train\"][\"label\"])\n",
    "Y_val = np.array(data[val_key][\"label\"])\n",
    "Y_test = np.array(data[test_key][\"label\"])\n",
    "\n",
    "if task_config.class_type == \"MC\":\n",
    "    Y_train = np.reshape(Y_train, (-1, 1))\n",
    "    Y_val = np.reshape(Y_val, (-1, 1))\n",
    "    Y_test = np.reshape(Y_test, (-1, 1))\n",
    "    \n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(Y_train)\n",
    "    print(ohe.categories_)\n",
    "     \n",
    "    Y_train = ohe.transform(Y_train)\n",
    "    Y_val = ohe.transform(Y_val)\n",
    "    Y_test = ohe.transform(Y_test)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings found!\n"
     ]
    }
   ],
   "source": [
    "cache_dir = f\"./cache/{embedding_param}/{task_param}\"\n",
    "cache_path = pathlib.Path(cache_dir)\n",
    "cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_names = ['X_train', 'X_val', 'X_test']\n",
    "paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
    "\n",
    "use_cached_embeddings = False\n",
    "if all(path.exists() for path in paths):\n",
    "    print(\"Saved embeddings found!\")\n",
    "    X_train = np.load(paths[0])\n",
    "    X_val = np.load(paths[1])\n",
    "    X_test = np.load(paths[2])\n",
    "    use_cached_embeddings = True\n",
    "else:\n",
    "    print(\"No saved embeddings found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cached_embeddings = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUESingleSentence(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "    \n",
    "class GLUEPairedSentence(Dataset):\n",
    "    def __init__(self, texts1, texts2):\n",
    "        self.texts1 = texts1\n",
    "        self.texts2 = texts2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1 = self.texts1[idx]\n",
    "        text2 = self.texts2[idx]\n",
    "\n",
    "        return text1, text2\n",
    "    \n",
    "def tokenize_batch(tokenizer, text_batch, max_len=512):\n",
    "    encoded = tokenizer(text_batch,\n",
    "                        add_special_tokens=True,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        max_length=max_len,\n",
    "                        return_tensors='pt')\n",
    "    return {k: v.to(device) for k, v in encoded.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_config.sentence_type == \"one\":\n",
    "    train_dataset = GLUESingleSentence(data['train']['sentence'])\n",
    "    val_dataset = GLUESingleSentence(data['validation']['sentence'])\n",
    "    test_dataset = GLUESingleSentence(data['test']['sentence'])\n",
    "    \n",
    "elif task_config.sentence_type == \"two\":\n",
    "    key1 = task_config.col_names[0]\n",
    "    key2 = task_config.col_names[1]\n",
    "    \n",
    "    train_dataset = GLUEPairedSentence(data['train'][key1], data['train'][key2])\n",
    "    val_dataset = GLUEPairedSentence(data[val_key][key1], data[val_key][key2])\n",
    "    test_dataset = GLUEPairedSentence(data[test_key][key1], data[test_key][key2])\n",
    "        \n",
    "else:\n",
    "    raise Exception(f\"{task_config.sentence_type}: sentence type not recognized\")\n",
    "\n",
    "# pick batch size based on GPU memory\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: Using default time tracking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58578c5354784a2e9dd5272d6447f59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a6f2587ae648b9b670bfd3aa5af8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d573178227a4e67a1908b5fea29cc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_single_embeddings(loader):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(loader):\n",
    "            encoded = tokenize_batch(tokenizer, text)\n",
    "            outputs = model(encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "\n",
    "            if embedding_param == \"cls\":\n",
    "                embed = extract_cls_embeddings(outputs)\n",
    "            elif embedding_param == \"mean_pooling\":\n",
    "                embed = mean_pooling(outputs, encoded[\"attention_mask\"])\n",
    "                \n",
    "            embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "def compute_pair_embeddings(loader):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for text1, text2 in tqdm(loader):\n",
    "            encoded1 = tokenize_batch(tokenizer, text1)\n",
    "            encoded2 = tokenize_batch(tokenizer, text2)\n",
    "\n",
    "            if embedding_param == \"sentence\":\n",
    "                U = torch.tensor(model.encode(encoded1[\"input_ids\"]))\n",
    "                V = torch.tensor(model.encode(encoded2[\"input_ids\"]))\n",
    "            else: # cls or mean_pooling\n",
    "                outputs1 = model(encoded1[\"input_ids\"], attention_mask=encoded1[\"attention_mask\"])\n",
    "                outputs2 = model(encoded2[\"input_ids\"], attention_mask=encoded2[\"attention_mask\"])\n",
    "\n",
    "                if embedding_param == \"cls\":\n",
    "                    U = extract_cls_embeddings(outputs1)\n",
    "                    V = extract_cls_embeddings(outputs2)\n",
    "                elif embedding_param == \"mean_pooling\":\n",
    "                    U = mean_pooling(outputs1, encoded1[\"attention_mask\"])\n",
    "                    V = mean_pooling(outputs2, encoded2[\"attention_mask\"])\n",
    "\n",
    "            embed = torch.cat([U, V], dim=1)\n",
    "            embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "if use_cached_embeddings == False:\n",
    "    if use_carbontracker:\n",
    "        print(\"Embedding: Using Carbontracker\")\n",
    "        tracker = CarbonTracker(epochs=1)\n",
    "        tracker.epoch_start()\n",
    "    else:\n",
    "        print(\"Embedding: Using default time tracking\")\n",
    "        # Track time\n",
    "        start_time = time.time()\n",
    "\n",
    "    if task_config.sentence_type == \"one\":\n",
    "        train_embed = compute_single_embeddings(train_loader)\n",
    "        val_embed = compute_single_embeddings(val_loader)\n",
    "        test_embed = compute_single_embeddings(test_loader)\n",
    "    elif task_config.sentence_type == \"two\":\n",
    "        train_embed = compute_pair_embeddings(train_loader)\n",
    "        val_embed = compute_pair_embeddings(val_loader)\n",
    "        test_embed = compute_pair_embeddings(test_loader)\n",
    "    \n",
    "    if use_carbontracker:\n",
    "        tracker.epoch_end()\n",
    "        embedding_time = 0.0 # temporary value for logging\n",
    "    else:\n",
    "        embedding_time = time.time() - start_time\n",
    "else:\n",
    "    embedding_time = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cached_embeddings == False:\n",
    "    X_train = train_embed\n",
    "    X_val = val_embed\n",
    "    X_test = test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute tensors for two sentence tasks\n",
    "def recompute_embedding(dataset):\n",
    "    U = dataset[:, :768]\n",
    "    V = dataset[:, 768:]\n",
    "    new = U - V\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 768)\n",
      "(71, 768)\n",
      "(146, 768)\n"
     ]
    }
   ],
   "source": [
    "recomputed = True\n",
    "X_train_recomputed = recompute_embedding(X_train)\n",
    "X_val_recomputed = recompute_embedding(X_val)\n",
    "X_test_recomputed = recompute_embedding(X_test)\n",
    "\n",
    "print(X_train_recomputed.shape)\n",
    "print(X_val_recomputed.shape)\n",
    "print(X_test_recomputed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving embeddings to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cached_embeddings == False:\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_names = ['X_train', 'X_val', 'X_test']\n",
    "    paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
    "\n",
    "    with open(paths[0], 'wb') as X_train_file:\n",
    "        np.save(X_train_file, X_train)\n",
    "    with open(paths[1], 'wb') as X_val_file:\n",
    "        np.save(X_val_file, X_val)\n",
    "    with open(paths[2], 'wb') as X_test_file:\n",
    "        np.save(X_test_file, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 hyperparameter combinations\n"
     ]
    }
   ],
   "source": [
    "input_size = task_config.input_size\n",
    "if recomputed:\n",
    "    input_size = task_config.input_size // 2\n",
    "\n",
    "param_grid = {\n",
    "    'num_epochs': [50],\n",
    "    'batch_size': [32, 512],\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'category': [task_config.class_type],\n",
    "    'norm': [False],\n",
    "    'input_size': [input_size],\n",
    "    'layer_size': [input_size // 4, input_size // 2, input_size, input_size * 2, input_size * 4],\n",
    "    'num_classes': [task_config.num_classes],\n",
    "    'num_layers': [1, 3, 5, 10],\n",
    "    'weight_decay':[1e-2, 1e-4],\n",
    "    'patience': [3],\n",
    "    'min_delta': [0],\n",
    "    'device': [device_name]\n",
    "}\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "print(f\"{len(all_params)} hyperparameter combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for logging\n",
    "console_output_filename = f'./output/{embedding_param}_{task_param}_console_output.txt'\n",
    "with open(console_output_filename, 'a') as logfile:\n",
    "    logfile.write('\\n\\nBEGIN TRAINING LOOP\\n\\n')\n",
    "\n",
    "# Setup for saving results\n",
    "results_folder = pathlib.Path(f\"results/{embedding_param}/{task_param}\")\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "save_file_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_folder / f\"val_{save_file_id}_{model_param}.csv\"\n",
    "\n",
    "if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "    with open(results_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        headers = list(all_params[0].keys())\n",
    "        writer.writerow(['mcc', 'f1', 'accuracy', 'training time', 'embedding time', 'training energy', 'embedding energy'] + headers)\n",
    "    print(f\"saving results to ./{results_file}\")\n",
    "    # Saves best accuracy for progress bar display\n",
    "    best_acc = 0.0\n",
    "elif task_config.class_type == \"R\":\n",
    "    with open(results_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        headers = list(all_params[0].keys())\n",
    "        writer.writerow(['pearson', 'spearman', 'training time', 'embedding time', 'training energy', 'embedding energy'] + headers)\n",
    "    print(f\"saving results to ./{results_file}\")\n",
    "    # Saves best accuracy for progress bar display\n",
    "    best_pearson = -2.0\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "bar = tqdm(enumerate(all_params), total=len(all_params))\n",
    "for i, params in bar:\n",
    "    # Formatting params to display\n",
    "    print_params = params.copy()\n",
    "    for param in ['category', 'device']:\n",
    "        del print_params[param]\n",
    "\n",
    "    # Initialize the model with current set of hyperparameters\n",
    "    feed_forward = FeedForward(**params)\n",
    "\n",
    "    if recomputed:\n",
    "        metrics, train_times_per_epoch, energy_per_epoch = feed_forward.fit(X_train_recomputed,\n",
    "                                                                            Y_train,\n",
    "                                                                            X_val_recomputed,\n",
    "                                                                            Y_val,\n",
    "                                                                            use_carbontracker)\n",
    "    else:\n",
    "        metrics, train_times_per_epoch, energy_per_epoch = feed_forward.fit(X_train,\n",
    "                                                                            Y_train,\n",
    "                                                                            X_val,\n",
    "                                                                            Y_val,\n",
    "                                                                            use_carbontracker)\n",
    "    \n",
    "    # Log average training time per epoch for current parameter set\n",
    "    # Note: FFN frequently stops early\n",
    "    training_time = np.mean(train_times_per_epoch)\n",
    "    training_energy = np.mean(energy_per_epoch) \n",
    "    # Compute energy for embedding generation\n",
    "    embedding_energy = get_energy(embedding_time, device) # This method effectively just computes energy for a given time\n",
    "\n",
    "    if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "        epoch, val_loss, val_accuracy, val_f1, val_mcc = metrics[\"epoch\"], metrics[\"loss\"], metrics[\"acc\"], metrics[\"f1\"], metrics[\"mcc\"]\n",
    "        best_acc = max(best_acc, val_accuracy)\n",
    "        bar.set_description(f\"Best Acc: {best_acc:.5f}, Last test: {val_accuracy:.5f}\")\n",
    "    \n",
    "        # Write stats to log file\n",
    "        with open(console_output_filename, 'a') as logfile:\n",
    "            logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
    "            logfile.write(f\"\\nEarly stopped on epoch: {epoch}\")\n",
    "            logfile.write(f\"\\nValidation accuracy: {val_accuracy}\")\n",
    "            logfile.write(f\"\\nValidation f1-score: {val_f1}\")\n",
    "            logfile.write(f\"\\nValidation MCC     : {val_mcc}\")\n",
    "            logfile.write(f\"\\nTraining time      : {training_time}\") \n",
    "            logfile.write(f\"\\nEmbedding time      : {embedding_time}\") \n",
    "            logfile.write(f\"\\nTraining energy    : {training_energy}\") \n",
    "            logfile.write(f\"\\nEmbedding energy    : {embedding_energy}\") \n",
    "        # Write to results csv\n",
    "        with open(results_file, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([val_mcc, val_f1, val_accuracy, training_time, embedding_time, training_energy, embedding_energy] + list(params.values()))\n",
    "                \n",
    "    elif task_config.class_type == \"R\":  # just report loss for regression task\n",
    "        epoch, val_loss, val_pearson, val_spearman = metrics[\"epoch\"], metrics[\"loss\"], metrics[\"pearson\"], metrics[\"spearman\"]\n",
    "        best_pearson = max(best_pearson, val_pearson)\n",
    "        bar.set_description(f\"Best Pearson's corrcoef: {best_pearson:.3f}, Last test: {val_pearson:.3f}\")\n",
    "    \n",
    "        # Write stats to log file\n",
    "        with open(console_output_filename, 'a') as logfile:\n",
    "            logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
    "            logfile.write(f\"\\nEarly stopped on epoch: {epoch}\")\n",
    "            logfile.write(f\"\\nValidation loss: {val_loss}\")\n",
    "            logfile.write(f\"\\nValidation Pearson's corrcoef: {val_pearson}\")\n",
    "            logfile.write(f\"\\nValidation Spearman's corrcoef: {val_spearman}\")\n",
    "            logfile.write(f\"\\nTraining time      : {training_time}\") \n",
    "            logfile.write(f\"\\nEmbedding time      : {embedding_time}\") \n",
    "            logfile.write(f\"\\nTraining energy    : {training_energy}\") \n",
    "            logfile.write(f\"\\nEmbedding energy    : {embedding_energy}\")  \n",
    "        # Write to results csv\n",
    "        with open(results_file, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([val_pearson, val_spearman, training_time, embedding_time, training_energy, embedding_energy] + list(params.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(results_file)\n",
    "# results_df = pd.read_csv(\"output/val_results_cola_20231127_151717.csv\")\n",
    "if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "    metric = \"accuracy\"\n",
    "    best = results_df[metric].max()\n",
    "    best_row = results_df[results_df[metric] == best]\n",
    "elif task_config.class_type == \"R\":\n",
    "    metric = \"pearson\"\n",
    "    best = results_df[metric].max()\n",
    "    best_row = results_df[results_df[metric] == best]\n",
    "\n",
    "best_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import pynvml\n",
    "from utils.adapter import BERTAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_param_grid = {\n",
    "    'num_epochs': [50],\n",
    "    'batch_size': [32, 512],\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'category': [task_config.class_type],\n",
    "    'device': [device_name]\n",
    "}\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "adapter_params = [dict(zip(adapter_param_grid.keys(), v)) for v in itertools.product(*adapter_param_grid.values())]\n",
    "print(f\"{len(adapter_params)} hyperparameter combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for saving power outputs\n",
    "results_folder = pathlib.Path(f\"results/adapter/{task_param}\")\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "save_file_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_folder / f\"val_{save_file_id}.csv\"\n",
    "with open(results_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['power', 'time'])\n",
    "print(f\"saving results to ./{results_file}\")\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "adapter_bar = tqdm(enumerate(adapter_params), total=len(adapter_params))\n",
    "for i, params in adapter_bar:\n",
    "    # Formatting params to display\n",
    "    print_params = params.copy()\n",
    "    for param in ['category', 'device']:\n",
    "        del print_params[param]\n",
    "\n",
    "    # Initialize the model with current set of hyperparameters\n",
    "    adapter = BERTAdapter(**params)\n",
    "\n",
    "    # Create a thread for tracking power output (NVIDIA ONLY), logs power and time\n",
    "    def get_power():\n",
    "        pynvml.nvmlInit()\n",
    "        while not adapter.train_stop_flag:\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Returns device handle, index 0 specifies the first GPU\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle)  # Retrieves power usage for GPU in milliwatts\n",
    "            power = power / 1000  # convert mW to W\n",
    "            curr_time = time.time()\n",
    "            print(\"Current NVIDIA power output:\", power, \"at time\", curr_time)\n",
    "            # Write to results csv\n",
    "            with open(results_file, 'a', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([power, curr_time])\n",
    "                \n",
    "        pynvml.nvmlShutdown()\n",
    "        \n",
    "    power_thread = threading.Thread(target=get_power)\n",
    "\n",
    "    # Make sure power output was recorded in the correct time range\n",
    "    print(\"Start time:\", time.time()) \n",
    "\n",
    "    if recomputed:\n",
    "        power_thread.start()  # Start the benchmarking thread\n",
    "        adapter.fit(X_train_recomputed,\n",
    "                    Y_train,\n",
    "                    X_val_recomputed,\n",
    "                    Y_val)\n",
    "        power_thread.join()  # Wait for the thread to complete\n",
    "    else:\n",
    "        power_thread.start()  # Start the benchmarking thread\n",
    "        adapter.fit(X_train,\n",
    "                    Y_train,\n",
    "                    X_val,\n",
    "                    Y_val)\n",
    "        power_thread.join()  # Wait for the thread to complete\n",
    "        \n",
    "    print(\"End time:\", time.time())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
