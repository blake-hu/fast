{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST: Feedforward-ASsisted Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for running GLUE tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import pathlib\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# from utils.feed_forward import FeedForward\n",
    "from utils.feed_forward import FeedForward\n",
    "from utils.cls import extract_cls_embeddings\n",
    "from utils.mean_pooling import mean_pooling\n",
    "from utils.energy import get_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUBLAS_WORKSPACE_CONFIG\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n",
    "os.environ['CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Standardized default seed\n",
    "seed = 7\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(mode=True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# device_name = \"cpu\"  # default device is CPU\n",
    "# if torch.cuda.is_available():\n",
    "#     # I read that this works for detecting if notebook is being run in a colab environment, not sure though\n",
    "#     if 'COLAB_GPU' in os.environ:\n",
    "#         print(\"colab environment\")\n",
    "#         device_name = \"gpu\" \n",
    "#     else:\n",
    "#         device_name = \"cuda\" # CUDA for NVIDIA GPU\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
    "\n",
    "device_name = \"cuda:0\"\n",
    "device = torch.device(device_name)\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to set:\n",
    "- Model\n",
    "    - MPNet\n",
    "    - DistilRoBERTa\n",
    "- Task\n",
    "    - cola\n",
    "    - sst2\n",
    "    - mrpc\n",
    "    - stsb\n",
    "    - qqp\n",
    "    - mnli_matched\n",
    "    - mnli_mismatched\n",
    "    - qnli\n",
    "    - rte\n",
    "    - wnli\n",
    "- Embedding type\n",
    "    - Single Sentence\n",
    "        - cls\n",
    "        - meanpool\n",
    "        - sentence\n",
    "    - Two Sentence\n",
    "        - Each sentence separately\n",
    "            - cls_single\n",
    "            - meanpool_single\n",
    "        - Both sentences at once\n",
    "            - cls_double\n",
    "            - meanpool_double\n",
    "        - sentence_single (no sentence_double option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = \"DistilRoBERTa\"\n",
    "task_param = \"mrpc\"\n",
    "embedding_param = [\"cls_single\"]\n",
    "\n",
    "use_carbontracker = False\n",
    "# NOTE: Carbontracker should generate its own output file \n",
    "# => logged benchmarking results will be dummy values if use_carbontracker = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if model_param == \"MPNet\":\n",
    "\n",
    "    if \"cls\" in \"_\".join(embedding_param) or \"meanpool\" in \"_\".join(embedding_param):\n",
    "        from transformers import MPNetTokenizer, MPNetModel\n",
    "        tokenizer = MPNetTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "        model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\").to(device)\n",
    "        \n",
    "    if \"sentence\" in \"_\".join(embedding_param):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        sentence_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "\n",
    "elif model_param == \"DistilRoBERTa\":\n",
    "\n",
    "    if \"cls\" in \"_\".join(embedding_param) or \"meanpool\" in \"_\".join(embedding_param):\n",
    "        from transformers import RobertaTokenizer, RobertaModel\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "        model = RobertaModel.from_pretrained('distilroberta-base').to(device)\n",
    "\n",
    "    if \"sentence\" in \"_\".join(embedding_param):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskConfig = namedtuple(\"TaskConfig\", [\"sentence_type\", \"class_type\", \"num_classes\", \"col_names\"])\n",
    "\n",
    "task_configs = {\n",
    "    \"cola\": TaskConfig(\"one\", \"BC\", 1, ['sentence']),\n",
    "    \"sst2\": TaskConfig(\"one\", \"BC\", 1, ['sentence']),\n",
    "    \"mrpc\": TaskConfig(\"two\", \"BC\", 1, ['sentence1', 'sentence2']),\n",
    "    \"stsb\": TaskConfig(\"two\", \"R\", 1, ['sentence1', 'sentence2']),\n",
    "    \"qqp\": TaskConfig(\"two\", \"BC\", 1, ['question1', 'question2']),\n",
    "    \"mnli_matched\": TaskConfig(\"two\", \"MC\", 3, ['premise', 'hypothesis']),\n",
    "    \"mnli_mismatched\": TaskConfig(\"two\", \"MC\", 3, ['premise', 'hypothesis']),\n",
    "    \"qnli\": TaskConfig(\"two\", \"BC\", 1, ['question', 'sentence']),\n",
    "    \"rte\": TaskConfig(\"two\", \"BC\", 1, ['sentence1', 'sentence2']),\n",
    "    \"wnli\": TaskConfig(\"two\", \"BC\", 1, ['sentence1', 'sentence2']),\n",
    "}\n",
    "\n",
    "task_config = task_configs[task_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if task_param == \"mnli_matched\": \n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_matched\"\n",
    "    test_key = \"test_matched\"\n",
    "elif task_param == \"mnli_mismatched\":\n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_mismatched\"\n",
    "    test_key = \"test_mismatched\"\n",
    "else:\n",
    "    data = load_dataset(\"glue\", task_param)\n",
    "    val_key = \"validation\"\n",
    "    test_key = \"test\"\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels come directly from dataset so no need to save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3668,)\n",
      "(408,)\n",
      "(1725,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Y_train = np.array(data[\"train\"][\"label\"])\n",
    "Y_val = np.array(data[val_key][\"label\"])\n",
    "Y_test = np.array(data[test_key][\"label\"])\n",
    "\n",
    "if task_config.class_type == \"MC\":\n",
    "    Y_train = np.reshape(Y_train, (-1, 1))\n",
    "    Y_val = np.reshape(Y_val, (-1, 1))\n",
    "    Y_test = np.reshape(Y_test, (-1, 1))\n",
    "    \n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(Y_train)\n",
    "    print(ohe.categories_)\n",
    "     \n",
    "    Y_train = ohe.transform(Y_train)\n",
    "    Y_val = ohe.transform(Y_val)\n",
    "    Y_test = ohe.transform(Y_test)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cls_single']\n",
      "cls_single embeddings found!\n"
     ]
    }
   ],
   "source": [
    "total_length = len(embedding_param)\n",
    "print(embedding_param)\n",
    "\n",
    "X_train, X_val, X_test, embedding_tracker = [None]*total_length, [None]*total_length, [None]*total_length, []\n",
    "create_dataloader, create_sentence = False, False\n",
    "\n",
    "for id, embedding in enumerate(embedding_param):\n",
    "    \n",
    "    cache_path = pathlib.Path(f\"./cache/{embedding}/{task_param}\")\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_names = ['X_train', 'X_val', 'X_test']\n",
    "    paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
    "\n",
    "    if all(path.exists() for path in paths):\n",
    "        print(f\"{embedding} embeddings found!\")\n",
    "        X_train[id] = np.load(paths[0])\n",
    "        X_val[id] = np.load(paths[1])\n",
    "        X_test[id] = np.load(paths[2])\n",
    "\n",
    "    else:\n",
    "        embedding_tracker.append(id)\n",
    "        print(f\"No {embedding} saved embeddings found\")\n",
    "\n",
    "        if \"cls\" in embedding_param[id] or \"meanpool\" in embedding_param[id]:\n",
    "            create_dataloader = True\n",
    "        elif \"sentence\" in embedding_param[id]:\n",
    "            create_sentence = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=512\n",
    "\n",
    "def tokenize(examples, name):\n",
    "    # print(task_config.col_names[0])\n",
    "    tokenized = tokenizer(examples[task_config.col_names[0]],\n",
    "                          add_special_tokens=True,\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=max_len,\n",
    "                          return_tensors='pt')\n",
    "    return {name + \"_\" + key: value.to(device) for key, value in tokenized.items()}\n",
    "\n",
    "def tokenize_single(examples, name):\n",
    "    # print(task_config.col_names[0], task_config.col_names[1])\n",
    "    tokenized_1 = tokenizer(examples[task_config.col_names[0]],\n",
    "                         add_special_tokens=True,\n",
    "                         padding='max_length',\n",
    "                         truncation=True,\n",
    "                         max_length=max_len,\n",
    "                         return_tensors='pt')\n",
    "    tokenized_2 = tokenizer(examples[task_config.col_names[1]],\n",
    "                         add_special_tokens=True,\n",
    "                         padding='max_length',\n",
    "                         truncation=True,\n",
    "                         max_length=max_len,\n",
    "                         return_tensors='pt')\n",
    "    \n",
    "    tokenized_output = {name + \"_\" + key + \"_1\": value.to(device) for key, value in tokenized_1.items()}\n",
    "    tokenized_output.update({name + \"_\" + key + \"_2\": value.to(device) for key, value in tokenized_2.items()})\n",
    "    return tokenized_output\n",
    "\n",
    "def tokenize_double(examples, name):\n",
    "    # print(task_config.col_names[0], task_config.col_names[1])\n",
    "    tokenized = tokenizer(examples[task_config.col_names[0]],\n",
    "                          examples[task_config.col_names[1]],\n",
    "                          add_special_tokens=True,\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=max_len,\n",
    "                          return_tensors='pt')\n",
    "    return {name + \"_\" + key: value.to(device) for key, value in tokenized.items()}\n",
    "\n",
    "inputs = []\n",
    "for id, embedding in enumerate(embedding_param):\n",
    "    if id in embedding_tracker and \"sentence\" not in embedding:\n",
    "        if \"single\" in embedding:\n",
    "            inputs.append(data.map(lambda examples: tokenize_single(examples, name=embedding), batched=True))\n",
    "        elif \"double\" in embedding:\n",
    "            inputs.append(data.map(lambda examples: tokenize_double(examples, name=embedding), batched=True))\n",
    "        else:\n",
    "            inputs.append(data.map(lambda examples: tokenize(examples, name=embedding), batched=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Function to merge datasets\n",
    "def clean_dataset(inputs, split):\n",
    "    common_columns_to_remove = {'sentence1', 'sentence2', 'label', 'question1', 'question2', 'premise', 'hypothesis', 'question'}\n",
    "\n",
    "    if isinstance(inputs, list):\n",
    "        dataframe = [ds[split].to_pandas() for ds in inputs]\n",
    "        dataframe = [df.drop(columns=common_columns_to_remove, errors='ignore') for df in dataframe]\n",
    "        cleaned_df = dataframe[0]\n",
    "        for df in dataframe[1:]:\n",
    "            cleaned_df = pd.merge(cleaned_df, df, on='idx', how='inner')\n",
    "    else:\n",
    "        dataframe = inputs[split].to_pandas()\n",
    "        cleaned_df = dataframe.drop(columns=common_columns_to_remove, errors='ignore')\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        # Exclude 'idx' column and convert others to tensors\n",
    "        self.columns = [col for col in dataframe.columns if col != 'idx']\n",
    "        for col in self.columns:\n",
    "            # Convert to NumPy array first for efficiency\n",
    "            np_array = np.array(dataframe[col].tolist())\n",
    "            setattr(self, col, torch.tensor(np_array, dtype=torch.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve tensors by index for each column\n",
    "        item = {col: getattr(self, col)[idx] for col in self.columns}\n",
    "        return item\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1024\n",
    "\n",
    "if create_dataloader:\n",
    "\n",
    "    # Merge datasets for each split and convert to PyTorch datasets\n",
    "    train_df = clean_dataset(inputs, 'train')\n",
    "    validation_df = clean_dataset(inputs, 'validation')\n",
    "    test_df = clean_dataset(inputs, 'test')\n",
    "\n",
    "    # Create dataset instances\n",
    "    train_dataset = TransformerDataset(train_df)\n",
    "    validation_dataset = TransformerDataset(validation_df)\n",
    "    test_dataset = TransformerDataset(test_df)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Embbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: Using default time tracking\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_embeddings(loader, X):\n",
    "    column_names = list(next(iter(loader)).keys())\n",
    "\n",
    "    # Creating pairs of input IDs and attention masks\n",
    "    pairs = []\n",
    "    for name in column_names:\n",
    "        if 'input_ids' in name:\n",
    "            mask_name = name.replace('input_ids', 'attention_mask')\n",
    "            pairs.append((name, mask_name))\n",
    "\n",
    "    # Loop over each pair of input ID and attention mask\n",
    "    for input_id_name, mask_name in pairs:\n",
    "        embedding = []\n",
    "        tqdm.write(f\"{input_id_name} {mask_name}\")\n",
    "        # time.sleep(0.2) # tqdm prints weird without slight time delay - delay subtracted when calculating embedding time\n",
    "    \n",
    "        for input_embedding in tqdm(loader):\n",
    "            # Move batch to device\n",
    "            model.eval()\n",
    "            input_embedding = {key: value.to(device) for key, value in input_embedding.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_embedding[input_id_name], attention_mask=input_embedding[mask_name])\n",
    "\n",
    "                if \"cls\" in input_id_name:\n",
    "                    embed = extract_cls_embeddings(outputs)\n",
    "                elif \"meanpool\" in input_id_name:\n",
    "                    embed = mean_pooling(outputs, input_embedding[mask_name])\n",
    "\n",
    "            embedding.append(embed.cpu().numpy())\n",
    "\n",
    "        id = embedding_param.index(input_id_name.split(\"_input_ids\")[0])\n",
    "        # print(id)\n",
    "        if \"2\" in input_id_name:\n",
    "            U, V = X[id], np.concatenate(embedding, axis=0)\n",
    "            X[id] = np.hstack([U, V])\n",
    "            # print(X[id])\n",
    "        else:\n",
    "            X[id] = np.concatenate(embedding, axis=0)\n",
    "            # print(X[id])\n",
    "\n",
    "def compute_sentence(sentences):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        batch_embeddings = sentence_model.encode(batch_sentences)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "if use_carbontracker:\n",
    "    print(\"Embedding: Using Carbontracker\")\n",
    "    tracker = CarbonTracker(epochs=1)\n",
    "    tracker.epoch_start()\n",
    "else:\n",
    "    print(\"Embedding: Using default time tracking\")\n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "\n",
    "if create_dataloader: # Compute CLS/Meanpool embeddings\n",
    "    compute_embeddings(train_loader, X_train)\n",
    "    compute_embeddings(validation_loader, X_val)\n",
    "    compute_embeddings(test_loader, X_test)\n",
    "if create_sentence: # Compute Sentence embeddings\n",
    "    if \"sentence_single\" in embedding_param:\n",
    "        id = embedding_param.index(\"sentence_single\")\n",
    "        U_train, V_train = compute_sentence(data[\"train\"][task_config.col_names[0]]), compute_sentence(data[\"train\"][task_config.col_names[1]])\n",
    "        U_val, V_val = compute_sentence(data[\"validation\"][task_config.col_names[0]]), compute_sentence(data[\"validation\"][task_config.col_names[1]])\n",
    "        U_test, V_test = compute_sentence(data[\"test\"][task_config.col_names[0]]), compute_sentence(data[\"test\"][task_config.col_names[1]])\n",
    "\n",
    "        X_train[id] = np.hstack([U_train, V_train])\n",
    "        X_val[id] = np.hstack([U_val, V_val])\n",
    "        X_test[id] = np.hstack([U_test, V_test])\n",
    "\n",
    "    elif \"sentence\" in embedding_param:\n",
    "        id = embedding_param.index(\"sentence\")\n",
    "        X_train[id] = compute_sentence(data[\"train\"][task_config.col_names[0]])\n",
    "        X_val[id] = compute_sentence(data[\"validation\"][task_config.col_names[0]])\n",
    "        X_test[id] = compute_sentence(data[\"test\"][task_config.col_names[0]])\n",
    "\n",
    "if use_carbontracker:\n",
    "    tracker.epoch_end()\n",
    "    embedding_time = 0.0 # temporary value for logging\n",
    "else:\n",
    "    embedding_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embeddings to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in embedding_tracker:\n",
    "\n",
    "    cache_path = pathlib.Path(f\"./cache/{embedding_param[id]}/{task_param}\")\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_names = ['X_train', 'X_val', 'X_test']\n",
    "    paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
    "\n",
    "    with open(paths[0], 'wb') as X_train_file:\n",
    "        np.save(X_train_file, X_train[id])\n",
    "    with open(paths[1], 'wb') as X_val_file:\n",
    "        np.save(X_val_file, X_val[id])\n",
    "    with open(paths[2], 'wb') as X_test_file:\n",
    "        np.save(X_test_file, X_test[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_val = np.concatenate(X_val, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(embeddings, column_ids, embedding_size, is_UV, is_diff, is_mult):\n",
    "    '''\n",
    "    Add embeddings at specific column_ids. For example, for the matrix\n",
    "    [Z1, Z2, U1, V1, Z3, U2, V2], if we want to replace this with:\n",
    "    [Z1, Z2, U1 - V1, Z3, U2, V2, U2 - V2, U2 * V2], we provide the parameters:\n",
    "    is_UV = [False, True] : U1, V1 are NOT kept, but U2, V2 are kept\n",
    "    is_diff = [True, True] : both U1 - V1 and U2 - V2 are added\n",
    "    is_mult = [False, True] : U1 * V1 is not included, U2 * V2 is included\n",
    "\n",
    "    Args:\n",
    "        embeddings : original matrix to replace\n",
    "        column_ids : location of Ux (we assume Vx immedeately follows Ux), \n",
    "                     for the above example, we would provide column_ids = [2, 5].\n",
    "                     If you DO NOT want to replace a certain Ux, simply don't include its id in column_ids\n",
    "        is_UV : should Ux, Vx be included\n",
    "        is_diff : should Ux - Vx be included\n",
    "        is_mult : should Ux * Vx be included\n",
    "    '''\n",
    "\n",
    "    id_delta = 0 # keep track of changes to embedding inserts/deletions\n",
    "    for id, column_id in enumerate(column_ids):\n",
    "\n",
    "        if id>0:\n",
    "            if not is_UV[id-1]:\n",
    "                id_delta -= 2\n",
    "            if is_diff[id-1]:\n",
    "                id_delta += 1\n",
    "            if is_mult[id-1]:\n",
    "                id_delta += 1\n",
    "\n",
    "        start_id = (column_id + id_delta) * embedding_size\n",
    "        U_id = start_id + embedding_size\n",
    "        V_id = start_id + (2*embedding_size)\n",
    "\n",
    "        U = embeddings[:, start_id:U_id]\n",
    "        V = embeddings[:, U_id:V_id]\n",
    "\n",
    "        if is_diff[id] and not is_mult[id]:\n",
    "            new_embeddings = np.abs(U - V)\n",
    "        elif not is_diff[id] and is_mult[id]:\n",
    "            new_embeddings = U * V\n",
    "        else: # both\n",
    "            new_embeddings = np.hstack([np.abs(U - V), (U * V)])\n",
    "\n",
    "        if is_UV[id]:                  \n",
    "            embeddings = np.hstack([\n",
    "                embeddings[:, :V_id],       # Part of original matrix before replacement\n",
    "                new_embeddings,             # New embeddings to insert\n",
    "                embeddings[:, V_id:]        # Part of original matrix after replacement\n",
    "            ])\n",
    "        else:\n",
    "            embeddings = np.hstack([\n",
    "                embeddings[:, :start_id],   # Part of original matrix before replacement\n",
    "                new_embeddings,             # New embeddings to insert\n",
    "                embeddings[:, V_id:]        # Part of original matrix after replacement\n",
    "            ])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cls_single']\n"
     ]
    }
   ],
   "source": [
    "print(embedding_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3668, 768)\n",
      "(408, 768)\n",
      "(1725, 768)\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Choose ids of (Ux, Vx) to alter embeddings\n",
    "column_ids = [0]\n",
    "######################\n",
    "\n",
    "if column_ids:\n",
    "    f = lambda embeddings : add_embeddings(embeddings=embeddings, column_ids=column_ids, embedding_size=768, \n",
    "                                           is_UV=[False], is_diff=[True], is_mult=[False])\n",
    "\n",
    "    X_train_computed = f(embeddings=X_train)\n",
    "    X_val_computed = f(embeddings=X_val)\n",
    "    X_test_computed = f(embeddings=X_test)\n",
    "else:\n",
    "    X_train_computed = X_train\n",
    "    X_val_computed = X_val\n",
    "    X_test_computed = X_test\n",
    "    \n",
    "print(X_train_computed.shape)\n",
    "print(X_val_computed.shape)\n",
    "print(X_test_computed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 hyperparameter combinations\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_computed.shape[1]\n",
    "\n",
    "\n",
    "# {\n",
    "#     'num_epochs': [50],\n",
    "#     'batch_size': [32, 128, 512],\n",
    "#     'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "#     'category': [task_config.class_type],\n",
    "#     'norm': [False],\n",
    "#     'input_size': [input_size],\n",
    "#     'layer_size': [100],\n",
    "#     'num_classes': [task_config.num_classes],\n",
    "#     'num_layers': [1, 2, 3, 5],\n",
    "#     'weight_decay':[1e-2, 1e-4],\n",
    "#     'patience': [3],\n",
    "#     'min_delta': [0],\n",
    "#     'device': [device_name]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'num_epochs': [100],\n",
    "    'batch_size': [512],\n",
    "    'learning_rate': [0.00001, 0.0001, 0.001],\n",
    "    'category': ['BC'],\n",
    "    'norm': [False],\n",
    "    'input_size': [768],\n",
    "    'layer_size': [1536],\n",
    "    'num_classes': [task_config.num_classes],\n",
    "    'num_layers': [10],\n",
    "    'weight_decay': [0.01],\n",
    "    'patience': [3],\n",
    "    'min_delta': [0],\n",
    "    'device': [device_name]\n",
    "    }\n",
    "\n",
    "# 'input_size': [input_size],\n",
    "# 'layer_size': [input_size // 4, input_size // 2, input_size, input_size * 2, input_size * 4],\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "print(f\"{len(all_params)} hyperparameter combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results to ./results/['cls_single']/mrpc/val_20240105_030147_DistilRoBERTa.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best: 0.75245, Last: 0.70343: 100%|██████████| 3/3 [00:11<00:00,  3.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Setup for logging\n",
    "console_output_filename = f'./output/{\"_\".join(embedding_param)}_{task_param}_console_output.txt'\n",
    "\n",
    "with open(console_output_filename, 'a') as logfile:\n",
    "    logfile.write('\\n\\nBEGIN TRAINING LOOP\\n\\n')\n",
    "\n",
    "# Setup for saving results\n",
    "results_folder = pathlib.Path(f\"results/{embedding_param}/{task_param}\")\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "save_file_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_folder / f\"val_{save_file_id}_{model_param}.csv\"\n",
    "\n",
    "# different metrics are recorded for classification vs regression tasks\n",
    "if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "    metric_types = ['mcc', 'f1', 'accuracy']\n",
    "elif task_config.class_type == \"R\":\n",
    "    metric_types = ['pearson', 'spearman']\n",
    "\n",
    "with open(results_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    header = metric_types + ['training time', 'embedding time', 'training energy', 'embedding energy'] + list(all_params[0].keys())\n",
    "    writer.writerow(header)\n",
    "print(f\"saving results to ./{results_file}\")\n",
    "# Saves best accuracy for progress bar display\n",
    "display_best = float(\"-inf\")\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "bar = tqdm(enumerate(all_params), total=len(all_params))\n",
    "for i, params in bar:\n",
    "    # Formatting params to display\n",
    "    print_params = params.copy()\n",
    "    for param in ['category', 'device']:\n",
    "        del print_params[param]\n",
    "\n",
    "    # reset torch so that results are consistent\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Initialize the model with current set of hyperparameters\n",
    "    feed_forward = FeedForward(**params)\n",
    "\n",
    "    metrics, train_times_per_epoch, energy_per_epoch = feed_forward.fit(X_train_computed,\n",
    "                                                                        Y_train,\n",
    "                                                                        X_val_computed,\n",
    "                                                                        Y_val,\n",
    "                                                                        use_carbontracker)\n",
    "    \n",
    "    # Log average training time per epoch for current parameter set\n",
    "    # Note: FFN frequently stops early\n",
    "    training_time = np.mean(train_times_per_epoch)\n",
    "    training_energy = np.mean(energy_per_epoch) \n",
    "    # Compute energy for embedding generation\n",
    "    embedding_energy = get_energy(embedding_time, device) # This method effectively just computes energy for a given time\n",
    "    \n",
    "    metric_vals = [metrics[mt] for mt in metric_types]\n",
    "    \n",
    "    # displaying results in progress bar\n",
    "    display_recent = metrics[\"pearson\" if task_config.class_type == \"R\" else \"accuracy\"]\n",
    "    display_best = max(display_best, display_recent)\n",
    "    bar.set_description(f\"Best: {display_best:.5f}, Last: {display_recent:.5f}\")\n",
    "\n",
    "    # Write stats to log file\n",
    "    with open(console_output_filename, 'a') as logfile:\n",
    "        logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
    "        logfile.write(f\"\\nEarly stopped on epoch: {metrics['epoch']}\")\n",
    "        for name, val in zip(metric_types, metric_vals):\n",
    "            logfile.write(f\"\\nValidation {name}: {val}\")\n",
    "        logfile.write(f\"\\nTraining time      : {training_time}\") \n",
    "        logfile.write(f\"\\nEmbedding time      : {embedding_time}\") \n",
    "        logfile.write(f\"\\nTraining energy    : {training_energy}\") \n",
    "        logfile.write(f\"\\nEmbedding energy    : {embedding_energy}\") \n",
    "    # Write to results csv\n",
    "    with open(results_file, 'a', newline='') as csvfile:                                                                \n",
    "        writer = csv.writer(csvfile)\n",
    "        row = metric_vals + [training_time, embedding_time, training_energy, embedding_energy] + list(params.values())\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcc</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training time</th>\n",
       "      <th>embedding time</th>\n",
       "      <th>training energy</th>\n",
       "      <th>embedding energy</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>category</th>\n",
       "      <th>norm</th>\n",
       "      <th>input_size</th>\n",
       "      <th>layer_size</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>patience</th>\n",
       "      <th>min_delta</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.426335</td>\n",
       "      <td>0.752451</td>\n",
       "      <td>0.752451</td>\n",
       "      <td>0.112964</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>8.472334</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>BC</td>\n",
       "      <td>False</td>\n",
       "      <td>768</td>\n",
       "      <td>1536</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426743</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>8.009259</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>BC</td>\n",
       "      <td>False</td>\n",
       "      <td>768</td>\n",
       "      <td>1536</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.193610</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.122419</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>9.181443</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>BC</td>\n",
       "      <td>False</td>\n",
       "      <td>768</td>\n",
       "      <td>1536</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda:0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mcc        f1  accuracy  training time  embedding time  \\\n",
       "0  0.426335  0.752451  0.752451       0.112964        0.000287   \n",
       "1  0.426743  0.750000  0.750000       0.106790        0.000287   \n",
       "2  0.193610  0.703431  0.703431       0.122419        0.000287   \n",
       "\n",
       "   training energy  embedding energy  num_epochs  batch_size  learning_rate  \\\n",
       "0         8.472334          0.021529         100         512        0.00001   \n",
       "1         8.009259          0.021529         100         512        0.00010   \n",
       "2         9.181443          0.021529         100         512        0.00100   \n",
       "\n",
       "  category   norm  input_size  layer_size  num_classes  num_layers  \\\n",
       "0       BC  False         768        1536            1          10   \n",
       "1       BC  False         768        1536            1          10   \n",
       "2       BC  False         768        1536            1          10   \n",
       "\n",
       "   weight_decay  patience  min_delta  device  \n",
       "0          0.01         3          0  cuda:0  \n",
       "1          0.01         3          0  cuda:0  \n",
       "2          0.01         3          0  cuda:0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv(results_file)\n",
    "# results_df = pd.read_csv(\"output/val_results_cola_20231127_151717.csv\")\n",
    "if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "    print_metric = \"accuracy\"\n",
    "elif task_config.class_type == \"R\":\n",
    "    print_metric = \"pearson\"\n",
    "\n",
    "best = results_df[print_metric].max()\n",
    "best_row = results_df[results_df[print_metric] == best]\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
