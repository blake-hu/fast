{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST: Feedforward-Augmented Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for running GLUE tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import csv\n",
    "import pathlib\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils.feed_forward import FeedForward\n",
    "from utils.cls import extract_cls_embeddings\n",
    "from utils.mean_pooling import mean_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized default seed\n",
    "seed = 7\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"  # default device is CPU\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"gpu\"  # CUDA for NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to set:\n",
    "- Model\n",
    "    - MPNetBase\n",
    "    - DistilRoBERTaBase\n",
    "    - MPNetST\n",
    "    - DistilRoBERTaST\n",
    "- Task\n",
    "    - cola\n",
    "    - sst2\n",
    "    - mrpc\n",
    "    - stsb\n",
    "    - qqp\n",
    "    - mnli_matched\n",
    "    - mnli_mismatched\n",
    "    - qnli\n",
    "    - rte\n",
    "    - wnli\n",
    "- Embedding type\n",
    "    - cls\n",
    "    - mean_pooling\n",
    "    - sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = \"DistilRoBERTaBase\"\n",
    "task_param = \"stsb\"\n",
    "embedding_param = \"cls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if model_param == \"MPNetBase\": # MPNet Base\n",
    "    from transformers import MPNetTokenizer, MPNetModel\n",
    "    tokenizer = MPNetTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\").to(device)\n",
    "elif model_param == \"DistilRoBERTaBase\": # DistilRoBERTa Base\n",
    "    from transformers import RobertaTokenizer, RobertaModel\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "    model = RobertaModel.from_pretrained('distilroberta-base').to(device)\n",
    "elif model_param == \"MPNetST\": # MPNet Sentence Transformer\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "elif model_param == \"DistilRoBERTaST\": # DistilRoBERTa Sentence Transformer\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-distilroberta-v1').to(device)\n",
    "else:\n",
    "    raise Exception(f\"ERROR: Bad model_param\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_type: [\"one\", \"two\"]\n",
    "# class_type: [\"BC\", \"MC\", \"R\"]\n",
    "# input_size: int (represents input size of feedforward, aka embedding size)\n",
    "# col_names: column names of relavent sentences on hugging face\n",
    "\n",
    "TaskConfig = namedtuple(\"TaskConfig\", [\"sentence_type\", \"class_type\", \"input_size\", \"col_names\"])\n",
    "task_configs = {\n",
    "    \"cola\": TaskConfig(\"one\", \"BC\", 768, ['sentence']),\n",
    "    \"sst2\": TaskConfig(\"one\", \"BC\", 768, ['sentence']),\n",
    "    \"mrpc\": TaskConfig(\"two\", \"BC\", 768*2, ['sentence1', 'sentence2']),\n",
    "    \"stsb\": TaskConfig(\"two\", \"R\", 768*2, ['sentence1', 'sentence2']),\n",
    "    \"qqp\": TaskConfig(\"two\", \"BC\", 768*2, ['question1', 'question2']),\n",
    "    \"mnli_matched\": TaskConfig(\"two\", \"MC\", 768*2, ['premise', 'hypothesis']),\n",
    "    \"mnli_mismatched\": TaskConfig(\"two\", \"MC\", 768*2, ['premise', 'hypothesis']),\n",
    "    \"qnli\": TaskConfig(\"two\", \"BC\", 768*2, ['question', 'sentence']),\n",
    "    \"rte\": TaskConfig(\"two\", \"BC\", 768*2, ['sentence1', 'sentence2']),\n",
    "    \"wnli\": TaskConfig(\"two\", \"BC\", 768*2, ['sentence1', 'sentence2']),\n",
    "}\n",
    "\n",
    "task_config = task_configs[task_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 5749\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if task_param == \"mnli_matched\": \n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_matched\"\n",
    "    test_key = \"test_matched\"\n",
    "elif task_param == \"mnli_mismatched\":\n",
    "    data = load_dataset(\"glue\", \"mnli\") \n",
    "    val_key = \"validation_mismatched\"\n",
    "    test_key = \"test_mismatched\"\n",
    "else:\n",
    "    data = load_dataset(\"glue\", task_param)\n",
    "    val_key = \"validation\"\n",
    "    test_key = \"test\"\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels come directly from dataset so no need to save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(data[\"train\"][\"label\"])\n",
    "Y_val = np.array(data[val_key][\"label\"])\n",
    "Y_test = np.array(data[test_key][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings found!\n"
     ]
    }
   ],
   "source": [
    "cache_dir = f\"./cache/{embedding_param}/{task_param}\"\n",
    "cache_path = pathlib.Path(cache_dir)\n",
    "cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_names = ['X_train', 'X_val', 'X_test']\n",
    "paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
    "\n",
    "use_cached_embeddings = False\n",
    "if all(path.exists() for path in paths):\n",
    "    print(\"Saved embeddings found!\")\n",
    "    X_train = np.load(paths[0])\n",
    "    X_val = np.load(paths[1])\n",
    "    X_test = np.load(paths[2])\n",
    "    use_cached_embeddings = True\n",
    "else:\n",
    "    print(\"No saved embeddings found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUESingleSentence(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)\n",
    "    \n",
    "class GLUEPairedSentence(Dataset):\n",
    "    def __init__(self, texts1, texts2, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts1 = texts1\n",
    "        self.texts2 = texts2\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1 = self.texts1[idx]\n",
    "        text2 = self.texts2[idx]\n",
    "\n",
    "        inputs1 = self.tokenizer.encode_plus(\n",
    "            text1, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs2 = self.tokenizer.encode_plus(\n",
    "            text2, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return inputs1['input_ids'].squeeze(0), inputs1['attention_mask'].squeeze(0), \\\n",
    "               inputs2['input_ids'].squeeze(0), inputs2['attention_mask'].squeeze(0)\n",
    "\n",
    "class GLUEPairedSentenceST(Dataset):\n",
    "    def __init__(self, texts1, texts2, max_length=512):\n",
    "        self.texts1 = texts1\n",
    "        self.texts2 = texts2\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs1 = self.texts1[idx]\n",
    "        inputs2 = self.texts2[idx]\n",
    "\n",
    "        return inputs1, inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_config.sentence_type == \"one\":\n",
    "    train_dataset = GLUESingleSentence(data['train']['sentence'], tokenizer)\n",
    "    val_dataset = GLUESingleSentence(data['validation']['sentence'], tokenizer)\n",
    "    test_dataset = GLUESingleSentence(data['test']['sentence'], tokenizer)\n",
    "    \n",
    "elif task_config.sentence_type == \"two\":\n",
    "    key1 = task_config.col_names[0]\n",
    "    key2 = task_config.col_names[1]\n",
    "    \n",
    "    if embedding_param in [\"cls\", \"mean_pooling\"]:\n",
    "        train_dataset = GLUEPairedSentence(data['train'][key1], data['train'][key2], tokenizer)\n",
    "        val_dataset = GLUEPairedSentence(data[val_key][key1], data[val_key][key2], tokenizer)\n",
    "        test_dataset = GLUEPairedSentence(data[test_key][key1], data[test_key][key2], tokenizer)\n",
    "    elif embedding_param == \"sentence\":\n",
    "        train_dataset = GLUEPairedSentenceST(data['train'][key1], data['train'][key2])\n",
    "        val_dataset = GLUEPairedSentenceST(data[val_key][key1], data[val_key][key2])\n",
    "        test_dataset = GLUEPairedSentenceST(data[test_key][key1], data[test_key][key2])\n",
    "        \n",
    "else:\n",
    "    raise Exception(f\"{task_config.sentence_type}: sentence type not recognized\")\n",
    "\n",
    "# pick batch size based on GPU memory\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_embeddings(loader):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            if embedding_param == \"cls\":\n",
    "                embed = extract_cls_embeddings(outputs)\n",
    "            elif embedding_param == \"mean_pooling\":\n",
    "                embed = mean_pooling(outputs, attention_mask)\n",
    "                \n",
    "            embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "def compute_pair_embeddings(loader):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if embedding_param in [\"cls\", \"mean_pooling\"]:\n",
    "            for input_ids1, attention_mask1, input_ids2, attention_mask2 in tqdm(loader):\n",
    "                input_ids1 = input_ids1.to(device)\n",
    "                attention_mask1 = attention_mask1.to(device)\n",
    "                input_ids2 = input_ids2.to(device)\n",
    "                attention_mask2 = attention_mask2.to(device)\n",
    "    \n",
    "                outputs1 = model(input_ids1, attention_mask=attention_mask1)\n",
    "                outputs2 = model(input_ids2, attention_mask=attention_mask2)\n",
    "                    \n",
    "                if embedding_param == \"cls\":\n",
    "                    U = extract_cls_embeddings(outputs1)\n",
    "                    V = extract_cls_embeddings(outputs2)\n",
    "                elif embedding_param == \"mean_pooling\":\n",
    "                    U = mean_pooling(outputs1, attention_mask1)\n",
    "                    V = mean_pooling(outputs2, attention_mask2)\n",
    "                    \n",
    "                embed = torch.cat([U, V], dim=1)\n",
    "                embeddings.append(embed.cpu().numpy())\n",
    "        \n",
    "        elif embedding_param == \"sentence\":\n",
    "            for input_ids1, input_ids2 in tqdm(loader):\n",
    "                U = torch.tensor(model.encode(input_ids1))\n",
    "                V = torch.tensor(model.encode(input_ids2))\n",
    "                embed = torch.cat([U, V], dim=1)\n",
    "                embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "if use_cached_embeddings == False:\n",
    "    if task_config.sentence_type == \"one\":\n",
    "        train_embed = compute_single_embeddings(train_loader)\n",
    "        val_embed = compute_single_embeddings(val_loader)\n",
    "        test_embed = compute_single_embeddings(test_loader)\n",
    "    elif task_config.sentence_type == \"two\":\n",
    "        train_embed = compute_pair_embeddings(train_loader)\n",
    "        val_embed = compute_pair_embeddings(val_loader)\n",
    "        test_embed = compute_pair_embeddings(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cached_embeddings == False:\n",
    "    X_train = train_embed\n",
    "    X_val = val_embed\n",
    "    X_test = test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute tensors\n",
    "def recompute_embedding(dataset):\n",
    "    recomputed = []\n",
    "\n",
    "    U = dataset[:, :768]\n",
    "    V = dataset[:, 768:]\n",
    "    new = U - V\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 768)\n",
      "(1500, 768)\n",
      "(1379, 768)\n"
     ]
    }
   ],
   "source": [
    "X_train_recomputed = recompute_embedding(X_train)\n",
    "X_val_recomputed = recompute_embedding(X_val)\n",
    "X_test_recomputed = recompute_embedding(X_test)\n",
    "\n",
    "print(X_train_recomputed.shape)\n",
    "print(X_val_recomputed.shape)\n",
    "print(X_test_recomputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 1536)\n",
      "(1500, 1536)\n",
      "(1379, 1536)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving embeddings to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cached_embeddings == False:\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_names = ['X_train', 'X_val', 'X_test']\n",
    "    paths = [pathlib.Path(cache_path / f\"{f}_{model_param}.npy\") for f in file_names]\n",
    "\n",
    "    with open(paths[0], 'wb') as X_train_file:\n",
    "        np.save(X_train_file, X_train)\n",
    "    with open(paths[1], 'wb') as X_val_file:\n",
    "        np.save(X_val_file, X_val)\n",
    "    with open(paths[2], 'wb') as X_test_file:\n",
    "        np.save(X_test_file, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 hyperparameter combinations\n"
     ]
    }
   ],
   "source": [
    "input_size = task_config.input_size\n",
    "\n",
    "param_grid = {\n",
    "    'num_epochs': [50],\n",
    "    'batch_size': [32, 512],\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'category': [task_config.class_type],\n",
    "    'norm': [False],\n",
    "    'input_size': [input_size],\n",
    "    'layer_size': [input_size // 2, input_size, input_size * 2],\n",
    "    'num_layers': [1, 5, 10],\n",
    "    'weight_decay':[1e-2, 1e-4],\n",
    "    'patience': [3],\n",
    "    'min_delta': [0],\n",
    "    'device': [device_name]\n",
    "}\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "print(f\"{len(all_params)} hyperparameter combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results to ./results/cls/stsb/val_20231215_094009_DistilRoBERTaBase.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best Pearson's corrcoef: 0.202, Last test: 0.196:   3%|█▏                                          | 2/72 [00:19<11:16,  9.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Initialize the model with current set of hyperparameters\u001b[39;00m\n\u001b[1;32m     38\u001b[0m feed_forward \u001b[38;5;241m=\u001b[39m FeedForward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 40\u001b[0m metrics, train_times_per_epoch, energy_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mfeed_forward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m training_time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_times_per_epoch)\n\u001b[1;32m     42\u001b[0m training_energy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(energy_per_epoch) \n",
      "File \u001b[0;32m~/Documents/TECH/research/projectx/projectx/fast/utils/feed_forward.py:247\u001b[0m, in \u001b[0;36mFeedForward.fit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m    244\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Perform optimization: Adjust each parameter by small step amount in direction of gradient\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     current_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# average_loss = sum(current_loss) / len(current_loss)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/fast/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/fast/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniforge3/envs/fast/lib/python3.9/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/fast/lib/python3.9/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fast/lib/python3.9/site-packages/torch/optim/adamw.py:383\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    380\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m    386\u001b[0m param\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup for logging\n",
    "console_output_filename = f'./output/{embedding_param}_{task_param}_console_output.txt'\n",
    "with open(console_output_filename, 'a') as logfile:\n",
    "    logfile.write('\\n\\nBEGIN TRAINING LOOP\\n\\n')\n",
    "\n",
    "# Setup for saving results\n",
    "results_folder = pathlib.Path(f\"results/{embedding_param}/{task_param}\")\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "save_file_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_folder / f\"val_{save_file_id}_{model_param}.csv\"\n",
    "\n",
    "if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "    with open(results_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        headers = list(all_params[0].keys())\n",
    "        writer.writerow(['mcc', 'f1', 'accuracy', 'training time', 'training energy'] + headers)\n",
    "    print(f\"saving results to ./{results_file}\")\n",
    "    # Saves best accuracy for progress bar display\n",
    "    best_acc = 0.0\n",
    "elif task_config.class_type == \"R\":\n",
    "    with open(results_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        headers = list(all_params[0].keys())\n",
    "        writer.writerow(['pearson', 'spearman', 'training time', 'training energy'] + headers)\n",
    "    print(f\"saving results to ./{results_file}\")\n",
    "    # Saves best accuracy for progress bar display\n",
    "    best_pearson = -2.0\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "bar = tqdm(enumerate(all_params), total=len(all_params))\n",
    "for i, params in bar:\n",
    "    # Formatting params to display\n",
    "    print_params = params.copy()\n",
    "    for param in ['category', 'device']:\n",
    "        del print_params[param]\n",
    "\n",
    "    # Initialize the model with current set of hyperparameters\n",
    "    feed_forward = FeedForward(**params)\n",
    "\n",
    "    metrics, train_times_per_epoch, energy_per_epoch = feed_forward.fit(X_train, Y_train, X_val, Y_val)\n",
    "    training_time = np.mean(train_times_per_epoch)\n",
    "    training_energy = np.mean(energy_per_epoch) \n",
    "    # Log average training time per epoch for current parameter set\n",
    "    # Note: FFN frequently stops early\n",
    "\n",
    "    if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "        epoch, val_loss, val_accuracy, val_f1, val_mcc = metrics[\"epoch\"], metrics[\"loss\"], metrics[\"acc\"], metrics[\"f1\"], metrics[\"mcc\"]\n",
    "        best_acc = max(best_acc, val_accuracy)\n",
    "        bar.set_description(f\"Best Acc: {best_acc:.5f}, Last test: {val_accuracy:.5f}\")\n",
    "    \n",
    "        # Write stats to log file\n",
    "        with open(console_output_filename, 'a') as logfile:\n",
    "            logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
    "            logfile.write(f\"\\nEarly stopped on epoch: {epoch}\")\n",
    "            logfile.write(f\"\\nValidation accuracy: {val_accuracy}\")\n",
    "            logfile.write(f\"\\nValidation f1-score: {val_f1}\")\n",
    "            logfile.write(f\"\\nValidation MCC     : {val_mcc}\")\n",
    "            logfile.write(f\"\\nTraining time      : {training_time}\") \n",
    "            logfile.write(f\"\\nTraining energy    : {training_energy}\") \n",
    "        # Write to results csv\n",
    "        with open(results_file, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([val_mcc, val_f1, val_accuracy, training_time, training_energy] + list(params.values()))\n",
    "            \n",
    "    elif task_config.class_type == \"R\":  # just report loss for regression task\n",
    "        epoch, val_loss, val_pearson, val_spearman = metrics[\"epoch\"], metrics[\"loss\"], metrics[\"pearson\"], metrics[\"spearman\"]\n",
    "        best_pearson = max(best_pearson, val_pearson)\n",
    "        bar.set_description(f\"Best Pearson's corrcoef: {best_pearson:.3f}, Last test: {val_pearson:.3f}\")\n",
    "    \n",
    "        # Write stats to log file\n",
    "        with open(console_output_filename, 'a') as logfile:\n",
    "            logfile.write(f\"\\n\\nTraining with parameters:\\n{print_params}\")\n",
    "            logfile.write(f\"\\nEarly stopped on epoch: {epoch}\")\n",
    "            logfile.write(f\"\\nValidation loss: {val_loss}\")\n",
    "            logfile.write(f\"\\nValidation Pearson's corrcoef: {val_pearson}\")\n",
    "            logfile.write(f\"\\nValidation Spearman's corrcoef: {val_spearman}\")\n",
    "            logfile.write(f\"\\nTraining time      : {training_time}\") \n",
    "            logfile.write(f\"\\nTraining energy    : {training_energy}\") \n",
    "        # Write to results csv\n",
    "        with open(results_file, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([val_pearson, val_spearman, training_time, training_energy] + list(params.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(results_file)\n",
    "# results_df = pd.read_csv(\"output/val_results_cola_20231127_151717.csv\")\n",
    "if task_config.class_type in [\"BC\", \"MC\"]:\n",
    "    metric = \"accuracy\"\n",
    "    best = results_df[metric].max()\n",
    "    best_row = results_df[results_df[metric] == best]\n",
    "elif task_config.class_type == \"R\":\n",
    "    metric = \"pearson\"\n",
    "    best = results_df[metric].max()\n",
    "    best_row = results_df[results_df[metric] == best]\n",
    "\n",
    "best_row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
