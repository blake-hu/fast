{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST: Feedforward-Augmented Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED IMPORTS & SETUP\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from utils.feed_forward import FeedForward\n",
    "from utils.cls import extract_cls_embeddings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set Device ##########################################################\n",
    "device_name = \"cpu\"  # default device is CPU\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda:0\"  # CUDA for NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
    "device = torch.device(device_name)\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models\n",
    "Load models from HuggingFace and send to GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnetv2 = SentenceTransformer(\"all-mpnet-base-v2\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"glue\", \"cola\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentence embeddings using sentence transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mpnetv2.encode(data[\"train\"][\"sentence\"])\n",
    "X_val = mpnetv2.encode(data[\"validation\"][\"sentence\"])\n",
    "X_test = mpnetv2.encode(data[\"test\"][\"sentence\"])\n",
    "\n",
    "Y_train = np.array(data[\"train\"][\"label\"])\n",
    "Y_val = np.array(data[\"validation\"][\"label\"])\n",
    "Y_test = np.array(data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save encodings and labels to disk for reuse. This is done because encoding embeddings takes a significant time but the encodings do not change throughout training, so we can cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/X_train.pt', 'wb') as X_train_file:\n",
    "    torch.save(X_train, X_train_file)\n",
    "with open('./output/X_val.pt', 'wb') as X_val_file:\n",
    "    torch.save(X_val, X_val_file)\n",
    "with open('./output/X_test.pt', 'wb') as X_test_file:\n",
    "    torch.save(X_test, X_test_file)\n",
    "with open('./output/Y_train.npy', 'wb') as Y_train_file:\n",
    "    np.save(Y_train_file, Y_train)\n",
    "with open('./output/Y_val.npy', 'wb') as Y_val_file:\n",
    "    np.save(Y_val_file, Y_val)\n",
    "with open('./output/Y_test.npy', 'wb') as Y_test_file:\n",
    "    np.save(Y_test_file, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved encodings and labels from disk, if previously saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/X_train.pt', 'rb') as X_train_file:\n",
    "    X_train = torch.load(X_train_file)\n",
    "with open('./output/X_val.pt', 'rb') as X_val_file:\n",
    "    X_val = torch.load(X_val_file)\n",
    "with open('./output/X_test.pt', 'rb') as X_test_file:\n",
    "    X_test = torch.load(X_test_file)\n",
    "with open('./output/Y_train.npy', 'rb') as Y_train_file:\n",
    "    Y_train = np.load(Y_train_file)\n",
    "with open('./output/Y_val.npy', 'rb') as Y_val_file:\n",
    "    Y_val = np.load(Y_val_file)\n",
    "with open('./output/Y_test.npy', 'rb') as Y_test_file:\n",
    "    Y_test = np.load(Y_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters\n",
    "Defining hyperparameter grid for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_epochs': [100],\n",
    "    'batch_size': [32, 128, 512],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    'category': ['C'],\n",
    "    'norm': [False],\n",
    "    'input_size': [768],\n",
    "    'layer_size': [768],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'weight_decay':[1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    'patience': [3],\n",
    "    'min_delta': [0],\n",
    "    'device': [device_name]\n",
    "}\n",
    "\n",
    "# Create a list of all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "highest_val_accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Hyperparameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all combinations of hyperparameters\n",
    "for params in all_params:\n",
    "    print(\"\\nTraining with parameters:\\n\", params)\n",
    "    # Initialize the model with current set of hyperparameters\n",
    "    feed_forward = FeedForward(num_epochs=params['num_epochs'],\n",
    "                                batch_size=params['batch_size'],\n",
    "                                learning_rate=params['learning_rate'],\n",
    "                                category=params['category'],\n",
    "                                norm=params['norm'],\n",
    "                                input_size=params['input_size'],\n",
    "                                layer_size=params['layer_size'],\n",
    "                                num_layers=params['num_layers'],\n",
    "                                weight_decay=params['weight_decay'],\n",
    "                                patience=params['patience'],\n",
    "                                min_delta=params['min_delta'],\n",
    "                                device=params['device'])\n",
    "    \n",
    "    epoch, val_loss, val_accuracy, val_f1, val_mcc = feed_forward.fit(X_train, Y_train, X_val, Y_val)\n",
    "    print(\"Early stopped on epoch:\", epoch)\n",
    "    print(\"Validation accuracy:\", val_accuracy)\n",
    "    print(\"Validation f1-score:\", val_f1)\n",
    "    print(\"Validation MCC     :\", val_mcc)\n",
    "    \n",
    "    # Save the parameters if they provide a better accuracy\n",
    "    if val_accuracy > highest_val_accuracy:\n",
    "        highest_val_accuracy = val_accuracy\n",
    "        best_params = params\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"Highest Validation Accuracy:\", highest_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feed_forward = FeedForward(num_epochs=best_params['num_epochs'],\n",
    "                                batch_size=best_params['batch_size'],\n",
    "                                learning_rate=best_params['learning_rate'],\n",
    "                                category=best_params['category'],\n",
    "                                norm=best_params['norm'],\n",
    "                                input_size=best_params['input_size'],\n",
    "                                layer_size=best_params['layer_size'],\n",
    "                                num_layers=best_params['num_layers'],\n",
    "                                weight_decay=best_params['weight_decay'],\n",
    "                                patience=best_params['patience'],\n",
    "                                min_delta=best_params['min_delta'],\n",
    "                                device=best_params['device'])\n",
    "\n",
    "X = np.concatenate((X_train, X_val), axis=0)\n",
    "Y = np.concatenate((Y_train, Y_val), axis=0)\n",
    "\n",
    "best_feed_forward.fit(X, Y)\n",
    "\n",
    "preds = np.argmax(best_feed_forward.predict_proba(X_test), axis=1)\n",
    "print(preds.shape)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'index': range(len(preds)),\n",
    "    'prediction': preds\n",
    "})\n",
    "\n",
    "# Write the DataFrame to a .tsv file, without the header and index\n",
    "df.to_csv('CoLA.tsv', sep='\\t', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
